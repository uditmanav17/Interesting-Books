{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch10 Intro to ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ayy1EbqEVke8",
        "bHPEkZgLVqJt",
        "FxieNIHoVtPS",
        "lKOcVJ_GVYUU",
        "G6beMzDHXaeO",
        "ew7b2pLxXn1x"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z4uukhCw3aX",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DrhadqIxvoi",
        "colab_type": "text"
      },
      "source": [
        "## From Biological to Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiTEW6T8x5NV",
        "colab_type": "text"
      },
      "source": [
        "### The Perceptron\n",
        "The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron called a ***threshold logic unit (TLU)***, or sometimes a ***linear threshold unit (LTU)***. The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight. The TLU computes a weighted sum of its inputs $(z = w_1 x_1 + w_2 x_2 + ... + w_n x_n = x^T w)$, then applies a step function to that sum and outputs the result: $h_w(x) = step(z)$, where $z = x^T w$.\n",
        "\n",
        "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), the layer is called a ***fully connected layer***, or a ***dense layer***. The inputs of the Perceptron are fed to special passthrough neurons called input neurons: they output whatever input they are fed. All the input neurons form the input layer. Moreover, an extra bias feature is generally added (x0 = 1): it is typically represented using a special type of neuron called a ***bias neuron***, which outputs 1 all the time.\n",
        "\n",
        "***Computing the outputs of a fully connected layer***<br>\n",
        "$h_{w, b} = \\phi(XW + b)$\n",
        "\n",
        "Where -\n",
        "* $X$ is input features matrix\n",
        "* $W$ is weight matrix\n",
        "* $B$ is bias\n",
        "* $\\phi$ is activation function\n",
        "\n",
        "So, how is a Perceptron trained? The Perceptron training algorithm proposed by Rosenblatt was largely inspired by Hebb’s rule. Donald Hebb suggested that when a biological neuron triggers another neuron often, the connection between these two neurons grows stronger. Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire together, wire together”; that is, the connection weight between two neurons tends to increase when they fire simultaneously. This rule later became known as Hebb’s rule (or Hebbian learning).\n",
        "\n",
        "***Perceptron Learning Rule (weight update)***<br>\n",
        "$w_{i, j}^{next step} = w_{i, j} + \\eta(y_j - \\hat{y}_j)x_i$\n",
        "\n",
        "where-\n",
        "* $w_{i, j}$ is connection bw ith input and jth output neuron\n",
        "* $x_i$ is the ith input value of current training instance\n",
        "* $\\hat{y}_j$ is the output of jth neuron for current training instance\n",
        "* $y_j$ is the target output of the jth output neuron for the current training instance\n",
        "* $\\eta$ is learning rate\n",
        "\n",
        "The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers). However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution. This is called the ***Perceptron convergence theorem***.\n",
        "\n",
        "Let's have a look\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhID1t7XfuMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)] # petal length, petal width\n",
        "y = (iris.target == 0).astype(np.int) # Iris setosa?"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KLSE_zJ4JUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "450ff3b7-1149-4d2a-b7cc-7fc86682ae2f"
      },
      "source": [
        "# creating and training perceptron\n",
        "per_clf = Perceptron()\n",
        "per_clf.fit(X, y)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
              "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
              "           validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQipxNFK4NeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3e990d6-8b3a-44cc-bd63-2210eec7b016"
      },
      "source": [
        "# predicting\n",
        "y_pred = per_clf.predict([[2, 0.5]])\n",
        "y_pred"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCwKrHGI4oLk",
        "colab_type": "text"
      },
      "source": [
        "Perceptron learning algorithm strongly resembles Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization).\n",
        "\n",
        "Contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. This is one reason to prefer Logistic Regression over Perceptrons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD3EjWls7DIY",
        "colab_type": "text"
      },
      "source": [
        "### MLP and Backpropagation\n",
        "An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layer. The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers.\n",
        "\n",
        "When an ANN contains a deep stack of hidden layers, it is called a deep neural network (DNN). The field of Deep Learning studies DNNs, and more generally models containing deep stacks of computations.\n",
        "\n",
        "For many years researchers struggled to find a way to train MLPs, without success. But in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a [groundbreaking paper](homl.info/44) that introduced the backpropagation training algorithm, which is still used today. In short, it is Gradient Descent using an efficient technique for computing the gradients automatically:11 in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
        "\n",
        "In order for this algorithm to work properly, its authors made a key change to the MLP’s architecture: they replaced the step function with the logistic (sigmoid) function, $σ(z) = \\frac{1}{1 + exp(–z)}$. This was essential because the step function contains only flat segments, so there is no gradient to work with (Gradient Descent cannot move on a flat surface), while the logistic function has a well-defined nonzero derivative everywhere, allowing Gradient Descent to make some progress at every step. In fact, the backpropagation algorithm works well with many other activation functions, not just the logistic function. Here are two other popular choices:\n",
        "\n",
        "* ***The hyperbolic tangent function: $tanh(z) = 2σ(2z) – 1$*** - Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
        "\n",
        "* ***The Rectified Linear Unit function: $ReLU(z) = max(0, z)$*** - The ReLU function is continuous but unfortunately not differentiable at z = 0 (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for z < 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.\n",
        "\n",
        "Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. For example, if $f(x) = 2x + 3$ and $g(x) = 5x – 1$, then chaining these two linear functions gives you another linear function: $f(g(x)) = 2(5x – 1) + 3 = 10x + 1$. So if you don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can’t solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX9WFH4tA94D",
        "colab_type": "text"
      },
      "source": [
        "### Regression MLPs\n",
        "\n",
        "MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension.\n",
        "\n",
        "The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can use the Huber loss, which is a combination of both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95n5-2KFCQm-",
        "colab_type": "text"
      },
      "source": [
        "### Classification MLPs\n",
        "\n",
        "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number.\n",
        "\n",
        "MLPs can also easily handle multilabel binary classification tasks. For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the logistic activation function: the first would output the probability that the email is spam, and the second would output the probability that it is urgent. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an error).\n",
        "\n",
        "If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activation function for the whole output layer. The softmax function will ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1 (which is required if the classes are exclusive). This is called multiclass classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KdTmJeoFzR8",
        "colab_type": "text"
      },
      "source": [
        "## Implementing MLP in Keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRc3brTLG1hB",
        "colab_type": "text"
      },
      "source": [
        "### Building an Image Classifier Using the Sequential API\n",
        "\n",
        "Here we will tackle Fashion MNIST. It has the exact same format as MNIST (70,000 grayscale images of 28 × 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse, and the problem turns out to be significantly more challenging than MNIST.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3FHek63IHar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5d68d75-363c-44e0-889d-102f9185366d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzMZW0FkVey8",
        "colab_type": "text"
      },
      "source": [
        "#### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHabBlQj4QpG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "a370eb2c-8ad0-4388-9d4c-05457d15cac8"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "print(f\"X_train_full.shape - {X_train_full.shape}\")\n",
        "print(f\"y_train_full.shape - {y_train_full.shape}\")\n",
        "print(f\"X_test.shape - {X_test.shape}\")\n",
        "print(f\"y_test.shape - {y_test.shape}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "X_train_full.shape - (60000, 28, 28)\n",
            "y_train_full.shape - (60000,)\n",
            "X_test.shape - (10000, 28, 28)\n",
            "y_test.shape - (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9HXBLP7IyD2",
        "colab_type": "text"
      },
      "source": [
        "The dataset is already split into a training set and a test set, but there is no validation set, so we’ll create one now. Additionally, since we are going to train the neural network using Gradient Descent, we must scale the input features. For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 (this also converts them to floats):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83hAIgAfIDgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLpn-u2bI4HJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class names\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "          "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L_NCoBiJIjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "b5f1b1d0-5e0c-4fe7-e2d3-d5ec24f08485"
      },
      "source": [
        "# check 1st example\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(class_names[y_train[0]]     )\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb927d936a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayy1EbqEVke8",
        "colab_type": "text"
      },
      "source": [
        "#### Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RamHkIwqJS6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5b014e7f-f619-4b7c-958a-7c7bb335f7a1"
      },
      "source": [
        "# creating model using sequential API\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2cT_myDKyzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4f348da2-5c51-4f22-9ba1-89d7fa8a9ae7"
      },
      "source": [
        "# another way\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRnAFT1vK2OI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ebe713af-dd3f-486e-afd3-0c110da507a8"
      },
      "source": [
        "# access model layers\n",
        "model.layers"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.core.Flatten at 0x7fb9278c2080>,\n",
              " <keras.layers.core.Dense at 0x7fb9278c2400>,\n",
              " <keras.layers.core.Dense at 0x7fb9278c24e0>,\n",
              " <keras.layers.core.Dense at 0x7fb9278d5358>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYzD-2_fOXJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6bfee571-f547-45c3-92bc-f660967940e2"
      },
      "source": [
        "# get model layer\n",
        "hidden1 = model.layers[1]\n",
        "print(hidden1.name)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense_4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y4vbTTEQDG3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "41a8fe64-ca7f-4004-b8db-687fd6190253"
      },
      "source": [
        "weights, biases = hidden1.get_weights()\n",
        "print(weights.shape)\n",
        "print(biases.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 300)\n",
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIMi97brQNZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6dce5e06-e2ca-461f-bbb2-63cec8f51db7"
      },
      "source": [
        "weights"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.04407284,  0.04791442,  0.00694428, ...,  0.07082225,\n",
              "         0.01623923,  0.03498274],\n",
              "       [-0.06899786, -0.06625619,  0.07091267, ..., -0.01564338,\n",
              "         0.019649  ,  0.02695377],\n",
              "       [-0.01397872, -0.03017611,  0.01725819, ...,  0.03885771,\n",
              "         0.02542134, -0.05851365],\n",
              "       ...,\n",
              "       [-0.027394  , -0.02100584,  0.01041127, ..., -0.05601118,\n",
              "         0.00650506, -0.03630669],\n",
              "       [-0.05497999,  0.06816782,  0.04407532, ..., -0.03845143,\n",
              "         0.04408857,  0.07382588],\n",
              "       [ 0.02215005,  0.00708732, -0.0164591 , ...,  0.04777616,\n",
              "        -0.01970708, -0.03581548]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA7MHMHKQibQ",
        "colab_type": "text"
      },
      "source": [
        "The Dense layer initialized the connection weights randomly, and the biases were initialized to zeros, which is fine. If you ever want to use a different initialization method, you can set kernel_initializer (kernel is another name for the matrix of connection weights) or bias_initializer when creating the layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHPEkZgLVqJt",
        "colab_type": "text"
      },
      "source": [
        "#### Compiling Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X-zWLXZQRqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "              optimizer=\"sgd\", \n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k8-i1ajRQ_5",
        "colab_type": "text"
      },
      "source": [
        "We use the \"sparse_categorical_cross entropy\" loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead. If we were doing binary classification (with one or more binary labels), then we would use the \"sigmoid\" (i.e., logistic) activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss. \n",
        "\n",
        "NOTE: When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=keras.optimiz ers.SGD(lr=???) to set the learning rate, rather than opti mizer=\"sgd\", which defaults to lr=0.01."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxieNIHoVtPS",
        "colab_type": "text"
      },
      "source": [
        "#### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXveJbZmRQk9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a29f7122-3d1a-42b3-d59e-2b0e1d515300"
      },
      "source": [
        "# training a model\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=30, \n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/30\n",
            "55000/55000 [==============================] - 8s 144us/step - loss: 0.7349 - accuracy: 0.7588 - val_loss: 0.5090 - val_accuracy: 0.8334\n",
            "Epoch 2/30\n",
            "55000/55000 [==============================] - 6s 106us/step - loss: 0.4955 - accuracy: 0.8280 - val_loss: 0.4684 - val_accuracy: 0.8364\n",
            "Epoch 3/30\n",
            "55000/55000 [==============================] - 6s 107us/step - loss: 0.4490 - accuracy: 0.8425 - val_loss: 0.4223 - val_accuracy: 0.8558\n",
            "Epoch 4/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.4210 - accuracy: 0.8514 - val_loss: 0.4084 - val_accuracy: 0.8586\n",
            "Epoch 5/30\n",
            "55000/55000 [==============================] - 6s 106us/step - loss: 0.3986 - accuracy: 0.8592 - val_loss: 0.3871 - val_accuracy: 0.8666\n",
            "Epoch 6/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.3824 - accuracy: 0.8661 - val_loss: 0.3870 - val_accuracy: 0.8644\n",
            "Epoch 7/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.3675 - accuracy: 0.8700 - val_loss: 0.3618 - val_accuracy: 0.8726\n",
            "Epoch 8/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.3554 - accuracy: 0.8740 - val_loss: 0.3514 - val_accuracy: 0.8786\n",
            "Epoch 9/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.3461 - accuracy: 0.8772 - val_loss: 0.3429 - val_accuracy: 0.8814\n",
            "Epoch 10/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.3364 - accuracy: 0.8785 - val_loss: 0.3521 - val_accuracy: 0.8728\n",
            "Epoch 11/30\n",
            "55000/55000 [==============================] - 6s 106us/step - loss: 0.3275 - accuracy: 0.8828 - val_loss: 0.3513 - val_accuracy: 0.8740\n",
            "Epoch 12/30\n",
            "55000/55000 [==============================] - 6s 106us/step - loss: 0.3198 - accuracy: 0.8844 - val_loss: 0.3281 - val_accuracy: 0.8838\n",
            "Epoch 13/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.3136 - accuracy: 0.8877 - val_loss: 0.3216 - val_accuracy: 0.8858\n",
            "Epoch 14/30\n",
            "55000/55000 [==============================] - 6s 106us/step - loss: 0.3052 - accuracy: 0.8905 - val_loss: 0.3241 - val_accuracy: 0.8862\n",
            "Epoch 15/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2991 - accuracy: 0.8924 - val_loss: 0.3206 - val_accuracy: 0.8890\n",
            "Epoch 16/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.2922 - accuracy: 0.8951 - val_loss: 0.3147 - val_accuracy: 0.8886\n",
            "Epoch 17/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2869 - accuracy: 0.8963 - val_loss: 0.3106 - val_accuracy: 0.8870\n",
            "Epoch 18/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2818 - accuracy: 0.8976 - val_loss: 0.3265 - val_accuracy: 0.8796\n",
            "Epoch 19/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.2760 - accuracy: 0.9009 - val_loss: 0.3103 - val_accuracy: 0.8888\n",
            "Epoch 20/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.2711 - accuracy: 0.9027 - val_loss: 0.3119 - val_accuracy: 0.8862\n",
            "Epoch 21/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2663 - accuracy: 0.9041 - val_loss: 0.3101 - val_accuracy: 0.8876\n",
            "Epoch 22/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2608 - accuracy: 0.9052 - val_loss: 0.3025 - val_accuracy: 0.8910\n",
            "Epoch 23/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2562 - accuracy: 0.9075 - val_loss: 0.2954 - val_accuracy: 0.8948\n",
            "Epoch 24/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2520 - accuracy: 0.9096 - val_loss: 0.2955 - val_accuracy: 0.8970\n",
            "Epoch 25/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.2470 - accuracy: 0.9114 - val_loss: 0.3083 - val_accuracy: 0.8874\n",
            "Epoch 26/30\n",
            "55000/55000 [==============================] - 6s 105us/step - loss: 0.2437 - accuracy: 0.9128 - val_loss: 0.3000 - val_accuracy: 0.8912\n",
            "Epoch 27/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.2399 - accuracy: 0.9140 - val_loss: 0.3030 - val_accuracy: 0.8882\n",
            "Epoch 28/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.2356 - accuracy: 0.9149 - val_loss: 0.3018 - val_accuracy: 0.8894\n",
            "Epoch 29/30\n",
            "55000/55000 [==============================] - 6s 103us/step - loss: 0.2319 - accuracy: 0.9161 - val_loss: 0.3079 - val_accuracy: 0.8896\n",
            "Epoch 30/30\n",
            "55000/55000 [==============================] - 6s 104us/step - loss: 0.2280 - accuracy: 0.9180 - val_loss: 0.2974 - val_accuracy: 0.8916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsnaSu7HT2P6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dcb611a2-3274-4291-88a1-beb3f36d3fa0"
      },
      "source": [
        "# evaluating model\n",
        "print(model.evaluate(X_train, y_train))\n",
        "print(model.evaluate(X_valid, y_valid))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55000/55000 [==============================] - 4s 68us/step\n",
            "[0.21797753107764503, 0.9216181635856628]\n",
            "5000/5000 [==============================] - 0s 65us/step\n",
            "[0.2974189668267965, 0.8916000127792358]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8MRPlqmRP3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6094de67-b588-4d35-ab3c-a4f610e1d1dd"
      },
      "source": [
        "# plotting training results\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBIt5g6QVUpb",
        "colab_type": "text"
      },
      "source": [
        "You can see that both the training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease. Good! Moreover, the validation curves are close to the training curves, which means that there is not too much overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training. But that’s not the case: indeed, the validation error is computed at the end of each epoch, while the training error is computed using a running mean during each epoch. So the training curve should be shifted by half an epoch to the left. If you do that, you will see that the training and validation curves overlap almost perfectly at the beginning of training.\n",
        "\n",
        "The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue training. It’s as simple as calling the fit() method again, since Keras just continues training where it left off (you should be able to reach close to 89% validation accuracy).\n",
        "\n",
        "f you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn’t help, try another optimizer (and always retune the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size (it can be set in the fit() method using the batch_size argument, which defaults to 32). \n",
        "\n",
        "It is common to get slightly lower performance on the test set than on the validation set, because the hyperparameters are tuned on the validation set, not the test set (however, in this example, we did not do any hyperparameter tuning, so the lower accuracy is just bad luck). Remember to resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the generalization error will be too optimistic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKOcVJ_GVYUU",
        "colab_type": "text"
      },
      "source": [
        "#### Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnnrEkpgUJXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2fe7b12b-b9da-446a-d310-f576a7de3e4e"
      },
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-r2lpD9WFzG",
        "colab_type": "text"
      },
      "source": [
        "If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the predict_classes() method instead:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnDKgZXiV1dB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b95572de-eee8-4eb5-b115-4be80fb44f50"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict_classes(X_new)\n",
        "print(f\"Predictions - {y_pred}\")\n",
        "print(f\"Predicted classes - {np.array(class_names)[y_pred]}\")\n",
        "print(f\"Actual classes - {np.array(class_names)[y_test[:3]]}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions - [9 2 1]\n",
            "Predicted classes - ['Ankle boot' 'Pullover' 'Trouser']\n",
            "Actual classes - ['Ankle boot' 'Pullover' 'Trouser']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lQupQXWvTl",
        "colab_type": "text"
      },
      "source": [
        "### Building a Regression MLP Using the Sequential API\n",
        "Let’s switch to the California housing problem and tackle it using a regression neural network. For simplicity, we will use Scikit-Learn’s fetch_california_housing() function to load the data. After loading the data, we split it into a training set, a validation set, and a test set, and we scale all the features:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6beMzDHXaeO",
        "colab_type": "text"
      },
      "source": [
        "#### Loading and prep data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3mJeg-cWM1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "44443cb2-8f11-49e8-b1d4-725531b64ca8"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew7b2pLxXn1x",
        "colab_type": "text"
      },
      "source": [
        "#### Build, compile, train, evaluate and test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIWMfcsTXeBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fc5c2f4c-c1fd-4898-b41b-42aa32450b1f"
      },
      "source": [
        "# build model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "\n",
        "# train model\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=20, \n",
        "                    validation_data=(X_valid, y_valid), \n",
        "                    verbose=0) # silent training\n",
        "\n",
        "# evaluate model\n",
        "print(f\"MSE - {model.evaluate(X_test, y_test)}\")\n",
        "\n",
        "# get predictions\n",
        "y_pred = model.predict(X_test[:3])\n",
        "print(f\"Predictions - {y_pred}\")\n",
        "print(f\"Actual Vals - {y_test[:3]}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 37us/step\n",
            "MSE - 0.34117018390995585\n",
            "Predictions - [[1.2308105]\n",
            " [2.3724523]\n",
            " [2.9295025]]\n",
            "Actual Vals - [1.516 1.919 3.118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V70Vz-QY7n0",
        "colab_type": "text"
      },
      "source": [
        "## Building Complex Models Using the Functional API\n",
        "\n",
        "One example of a nonsequential neural network is a Wide & Deep neural network. This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng et al. It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations.\n",
        "\n",
        "Let’s build such a neural network to tackle the California housing problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XqKsnMqYZuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "d63907b4-3cbd-459e-f41c-3904d1572a4b"
      },
      "source": [
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.Concatenate()([input_, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 8)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 30)           270         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 30)           930         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 38)           0           input_1[0][0]                    \n",
            "                                                                 dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            39          concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,239\n",
            "Trainable params: 1,239\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjStvUw0bboj",
        "colab_type": "text"
      },
      "source": [
        "Once you have built the Keras model, everything is exactly like earlier, so there’s no need to repeat it here: you must compile the model, train it, evaluate it, and use it to make predictions.\n",
        "\n",
        "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjPjQ6y2bDZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "ecfb609e-7a10-4f3e-ca73-178d809caeb9"
      },
      "source": [
        "# create layers\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
        "\n",
        "# build model\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "deep_input (InputLayer)         (None, 6)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 30)           210         deep_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "wide_input (InputLayer)         (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 30)           930         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 35)           0           wide_input[0][0]                 \n",
            "                                                                 dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            36          concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,176\n",
            "Trainable params: 1,176\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7asa-6Ybh7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "6c102b3d-3012-4bb0-b631-30ece33a0978"
      },
      "source": [
        "# plot model\n",
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5emMzKnb1sj",
        "colab_type": "text"
      },
      "source": [
        "The code is self-explanatory. You should name at least the most important layers, especially when the model gets a bit complex like this. Note that we specified inputs=[input_A, input_B] when creating the model. Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p51kmeWudZkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile model\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PrZD3hLdpur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prep data\n",
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F-n8ol2bntG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "bf5cdc62-7cc2-4fc8-fd81-7f94848adba9"
      },
      "source": [
        "# train model\n",
        "history = model.fit([X_train_A, X_train_B], y_train, \n",
        "                    epochs=20, \n",
        "                    validation_data=([X_valid_A, X_valid_B], y_valid), \n",
        "                    verbose=1)\n",
        "\n",
        "print(f\"MSE - {model.evaluate([X_test_A, X_test_B], y_test)}\")\n",
        "y_pred = model.predict([X_new_A, X_new_B])\n",
        "print(f\"Predictions - {y_pred}\")\n",
        "print(f\"Actual - {y_test[:3]}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/20\n",
            "11610/11610 [==============================] - 1s 97us/step - loss: 2.2943 - val_loss: 0.9358\n",
            "Epoch 2/20\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.7434 - val_loss: 0.6680\n",
            "Epoch 3/20\n",
            "11610/11610 [==============================] - 1s 93us/step - loss: 0.6270 - val_loss: 0.6001\n",
            "Epoch 4/20\n",
            "11610/11610 [==============================] - 1s 93us/step - loss: 0.5906 - val_loss: 0.5622\n",
            "Epoch 5/20\n",
            "11610/11610 [==============================] - 1s 91us/step - loss: 0.5684 - val_loss: 0.5425\n",
            "Epoch 6/20\n",
            "11610/11610 [==============================] - 1s 94us/step - loss: 0.5531 - val_loss: 0.5267\n",
            "Epoch 7/20\n",
            "11610/11610 [==============================] - 1s 95us/step - loss: 0.5381 - val_loss: 0.5114\n",
            "Epoch 8/20\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.5215 - val_loss: 0.4972\n",
            "Epoch 9/20\n",
            "11610/11610 [==============================] - 1s 91us/step - loss: 0.5114 - val_loss: 0.4912\n",
            "Epoch 10/20\n",
            "11610/11610 [==============================] - 1s 93us/step - loss: 0.5011 - val_loss: 0.4761\n",
            "Epoch 11/20\n",
            "11610/11610 [==============================] - 1s 94us/step - loss: 0.4909 - val_loss: 0.4701\n",
            "Epoch 12/20\n",
            "11610/11610 [==============================] - 1s 93us/step - loss: 0.4832 - val_loss: 0.4672\n",
            "Epoch 13/20\n",
            "11610/11610 [==============================] - 1s 94us/step - loss: 0.4759 - val_loss: 0.4546\n",
            "Epoch 14/20\n",
            "11610/11610 [==============================] - 1s 93us/step - loss: 0.4702 - val_loss: 0.4549\n",
            "Epoch 15/20\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.4649 - val_loss: 0.4480\n",
            "Epoch 16/20\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.4604 - val_loss: 0.4471\n",
            "Epoch 17/20\n",
            "11610/11610 [==============================] - 1s 92us/step - loss: 0.4558 - val_loss: 0.4432\n",
            "Epoch 18/20\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.4527 - val_loss: 0.4352\n",
            "Epoch 19/20\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.4494 - val_loss: 0.4400\n",
            "Epoch 20/20\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.4468 - val_loss: 0.4318\n",
            "5160/5160 [==============================] - 0s 38us/step\n",
            "MSE - 0.41746129357768585\n",
            "Predictions - [[1.0175116]\n",
            " [2.3091   ]\n",
            " [2.658202 ]]\n",
            "Actual - [1.516 1.919 3.118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq0U4DZPhbGx",
        "colab_type": "text"
      },
      "source": [
        "Adding extra outputs is quite easy: just connect them to the appropriate layers and add them to your model’s list of outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9k5yPkWcVM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "394dce7c-e738-4fe3-e070-1687c2bca1ae"
      },
      "source": [
        "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
        "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "deep_input (InputLayer)         (None, 6)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 30)           210         deep_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "wide_input (InputLayer)         (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 30)           930         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 35)           0           wide_input[0][0]                 \n",
            "                                                                 dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "main_output (Dense)             (None, 1)            36          concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "aux_output (Dense)              (None, 1)            31          dense_13[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,207\n",
            "Trainable params: 1,207\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJI08elehu-9",
        "colab_type": "text"
      },
      "source": [
        "Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses (if we pass a single loss, Keras will assume that the same loss must be used for all outputs). By default, Keras will compute all these losses and simply add them up to get the final loss used for training. We care much more about the main output than about the auxiliary output (as it is just used for regularization), so we want to give the main output’s loss a much greater weight. Fortunately, it is possible to set all the loss weights when compiling the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EUu_gBlhgvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGThAUgViGaH",
        "colab_type": "text"
      },
      "source": [
        "Now when we train the model, we need to provide labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing y_train, we need to pass (y_train, y_train) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGYCCsr2iF9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "history = model.fit([X_train_A, X_train_B], \n",
        "                    [y_train, y_train], \n",
        "                    epochs=20, \n",
        "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]), \n",
        "                    verbose=0)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mGJrd3UiYFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "80d519cf-0980-4c9c-d19b-45d41fac2494"
      },
      "source": [
        "# evaluate\n",
        "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])\n",
        "print(f\"total_loss: {total_loss} \\nmain_loss: {main_loss} \\naux_loss: {aux_loss}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 62us/step\n",
            "total_loss: 0.35381870766480766 \n",
            "main_loss: 0.3388870358467102 \n",
            "aux_loss: 0.4813053011894226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQgWS5MwioBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "384f807a-0945-4c2b-c747-dbc749a0ee02"
      },
      "source": [
        "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
        "print(f\"y_pred_main: {y_pred_main} \\ny_pred_aux: {y_pred_aux}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_pred_main: [[1.1140602]\n",
            " [2.2595751]\n",
            " [2.7978716]] \n",
            "y_pred_aux: [[1.7057779]\n",
            " [2.0408957]\n",
            " [2.86755  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhLuz_ZCjFGz",
        "colab_type": "text"
      },
      "source": [
        "## Using the Subclassing API to Build Dynamic Models\n",
        "\n",
        "Simply subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the Functional API. You can then compile it, evaluate it, and use it to make predictions, exactly like we just did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n6p4i1pixj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WideAndDeepModel(keras.Model):\n",
        "    \n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
        "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
        "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
        "        self.main_output = keras.layers.Dense(1)\n",
        "        self.aux_output = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_A, input_B = inputs\n",
        "        hidden1 = self.hidden1(input_B)\n",
        "        hidden2 = self.hidden2(hidden1)\n",
        "        concat = keras.layers.concatenate([input_A, hidden2])\n",
        "        main_output = self.main_output(concat)\n",
        "        aux_output = self.aux_output(hidden2)\n",
        "        return main_output, aux_output\n",
        "\n",
        "\n",
        "# model = WideAndDeepModel()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBOcWPp0nUv7",
        "colab_type": "text"
      },
      "source": [
        "This example looks very much like the Functional API, except we do not need to create the inputs; we just use the input argument to the call() method, and we separate the creation of the layers21 in the constructor from their usage in the call() method. The big difference is that you can do pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations—your imagination is the limit.\n",
        "\n",
        "This extra flexibility does come at a cost: your model’s architecture is hidden within the call() method, so Keras cannot easily inspect it; it cannot save or clone it; and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. So unless you really need that extra flexibility, you should probably stick to the Sequential API or the Functional API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7f0WRYin1cL",
        "colab_type": "text"
      },
      "source": [
        "## Saving and Restoring a Model\n",
        "\n",
        "When using the Sequential API or the Functional API, saving a trained Keras model is as simple as it gets:\n",
        "```Python\n",
        "model = keras.models.Sequential([...]) # or keras.Model([...])\n",
        "model.compile([...])\n",
        "model.fit([...])\n",
        "model.save(\"my_keras_model.h5\")\n",
        "```\n",
        "\n",
        "Keras will use the HDF5 format to save both the model’s architecture (including every layer’s hyperparameters) and the values of all the model parameters for every layer (e.g., connection weights and biases). It also saves the optimizer (including its hyperparameters and any state it may have). \n",
        "\n",
        "\n",
        "You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to make predictions. Loading the model is just as easy:\n",
        "\n",
        "```Python\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")\n",
        "```\n",
        "\n",
        "WARNING: This will work when using the Sequential API or the Functional API, but unfortunately not when using model subclassing. You can use save_weights() and load_weights() to at least save and restore the model parameters, but you will need to save and restore everything else yourself.\n",
        "\n",
        "But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes. But how can you tell the fit() method to save checkpoints? Use callbacks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDdMYLYKqM5q",
        "colab_type": "text"
      },
      "source": [
        "### Using Callbacks\n",
        "\n",
        "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:\n",
        "\n",
        "```Python\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
        "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
        "```\n",
        "\n",
        "If you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set. The following code is a simple way to implement early stopping:\n",
        "\n",
        "```Python\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True)\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb])\n",
        "model = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model\n",
        "```\n",
        "\n",
        "Another way to implement early stopping is to simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. You can combine both callbacks to save checkpoints of your model (in case your computer crashes) and interrupt training early when there is no more progress (to avoid wasting time and resources):\n",
        "\n",
        "```Python\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the EarlyStopping callback will keep track of the best weights and restore them for you at the end of training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SAYc-aCIWZy",
        "colab_type": "text"
      },
      "source": [
        "## Fine Tuning Neural Network\n",
        "\n",
        "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a simple MLP you can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. How do you know what combination of hyperparameters is the best for your task?\n",
        "\n",
        "One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross-validation). For example, we can use GridSearchCV or RandomizedSearchCV to explore the hyperparameter space. To do this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors. The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSyebwa4nNSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh4L5x6DTxHU",
        "colab_type": "text"
      },
      "source": [
        "This function creates a simple Sequential model for univariate regression (only one output neuron), with the given input shape and the given number of hidden layers and neurons, and it compiles it using an SGD optimizer configured with the specified learning rate. It is good practice to provide reasonable defaults to as many hyperparameters as you can, as Scikit-Learn does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYttrJUNOyQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a KerasRegressor based on this build_model() function\n",
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUDbGa_GUF4L",
        "colab_type": "text"
      },
      "source": [
        "The KerasRegressor object is a thin wrapper around the Keras model built using build_model(). Since we did not specify any hyperparameters when creating it, it will use the default hyperparameters we defined in build_model(). Now we can use this object like a regular Scikit-Learn regressor: we can train it using its fit() method, then evaluate it using its score() method, and use it to make predictions using its predict() method, as you can see in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f1w-q5YUGLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87de415d-54a8-4be6-d551-7df46c0301be"
      },
      "source": [
        "keras_reg.fit(X_train, y_train, epochs=100,\n",
        "              validation_data=(X_valid, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 83us/step - loss: 1.3851 - val_loss: 0.8391\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.6766 - val_loss: 1.0892\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 85us/step - loss: 0.6191 - val_loss: 0.5781\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.5677 - val_loss: 0.5354\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 79us/step - loss: 0.5235 - val_loss: 0.4827\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.4994 - val_loss: 0.5671\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 83us/step - loss: 0.4786 - val_loss: 0.4462\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 89us/step - loss: 0.4664 - val_loss: 0.4566\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 91us/step - loss: 0.4547 - val_loss: 0.4225\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 90us/step - loss: 0.4466 - val_loss: 0.4859\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 89us/step - loss: 0.4415 - val_loss: 0.4102\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 91us/step - loss: 0.4365 - val_loss: 0.5038\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.4304 - val_loss: 0.3994\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.4255 - val_loss: 0.4584\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.4214 - val_loss: 0.3915\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 79us/step - loss: 0.4169 - val_loss: 0.3919\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.4137 - val_loss: 0.4037\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 79us/step - loss: 0.4104 - val_loss: 0.4110\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.4081 - val_loss: 0.3835\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.4054 - val_loss: 0.3953\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.4018 - val_loss: 0.4124\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 79us/step - loss: 0.4022 - val_loss: 0.3731\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3987 - val_loss: 0.4047\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 78us/step - loss: 0.3965 - val_loss: 0.3725\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 85us/step - loss: 0.3945 - val_loss: 0.3819\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.3918 - val_loss: 0.3905\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 79us/step - loss: 0.3915 - val_loss: 0.3672\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 83us/step - loss: 0.3888 - val_loss: 0.4236\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 83us/step - loss: 0.3887 - val_loss: 0.3693\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.3874 - val_loss: 0.3825\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.3847 - val_loss: 0.3886\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3839 - val_loss: 0.3602\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3814 - val_loss: 0.4267\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 83us/step - loss: 0.3820 - val_loss: 0.3587\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.3797 - val_loss: 0.3768\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.3781 - val_loss: 0.3819\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3790 - val_loss: 0.3786\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3772 - val_loss: 0.3552\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.3777 - val_loss: 0.4312\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3753 - val_loss: 0.3494\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 87us/step - loss: 0.3763 - val_loss: 0.3864\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 83us/step - loss: 0.3766 - val_loss: 0.3645\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.3755 - val_loss: 0.3627\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 82us/step - loss: 0.3734 - val_loss: 0.3693\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 79us/step - loss: 0.3715 - val_loss: 0.3584\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3709 - val_loss: 0.3696\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 83us/step - loss: 0.3726 - val_loss: 0.3565\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 79us/step - loss: 0.3858 - val_loss: 0.3593\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 80us/step - loss: 0.3713 - val_loss: 0.3592\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 81us/step - loss: 0.3695 - val_loss: 0.3622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb9201a0a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCqPtl8RUVNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "56ca1c91-6d6f-4604-ee0d-df008142c38d"
      },
      "source": [
        "# check MSE\n",
        "mse_test = keras_reg.score(X_test, y_test)\n",
        "mse_test"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 36us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.34777579104253487"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3TTT_VU7KQ",
        "colab_type": "text"
      },
      "source": [
        "Any extra parameter you pass to the fit() method will get passed to the underlying Keras model. Also note that the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e., higher should be better).\n",
        "\n",
        "We don’t want to train and evaluate a single model like this, though we want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzfFhN1OUnra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b79fd4c-17d2-45b2-be7b-73b818f5b113"
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] learning_rate=0.0015411627181247465, n_hidden=2, n_neurons=14 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 1.9317 - val_loss: 1.0265\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.8064 - val_loss: 0.7994\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.7207 - val_loss: 0.7264\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.6832 - val_loss: 0.6797\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.6551 - val_loss: 0.6459\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.6314 - val_loss: 0.6195\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.6107 - val_loss: 0.5958\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.5910 - val_loss: 0.5764\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.5734 - val_loss: 0.5577\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.5571 - val_loss: 0.5416\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.5413 - val_loss: 0.5271\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.5265 - val_loss: 0.5088\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 110us/step - loss: 0.5129 - val_loss: 0.4967\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 111us/step - loss: 0.5001 - val_loss: 0.4844\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4882 - val_loss: 0.4741\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4778 - val_loss: 0.4617\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.4690 - val_loss: 0.4563\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.4610 - val_loss: 0.4492\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4546 - val_loss: 0.4408\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 109us/step - loss: 0.4489 - val_loss: 0.4386\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4440 - val_loss: 0.4302\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4395 - val_loss: 0.4270\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4355 - val_loss: 0.4233\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4313 - val_loss: 0.4198\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4279 - val_loss: 0.4173\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4239 - val_loss: 0.4140\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4200 - val_loss: 0.4117\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4174 - val_loss: 0.4069\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4146 - val_loss: 0.4049\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4121 - val_loss: 0.4045\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4095 - val_loss: 0.4017\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4074 - val_loss: 0.4032\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4057 - val_loss: 0.4018\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4042 - val_loss: 0.3987\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4023 - val_loss: 0.3976\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4009 - val_loss: 0.3964\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3998 - val_loss: 0.3980\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3983 - val_loss: 0.3986\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3970 - val_loss: 0.3965\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3963 - val_loss: 0.3953\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3948 - val_loss: 0.3928\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3937 - val_loss: 0.3916\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3929 - val_loss: 0.3938\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3918 - val_loss: 0.3907\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3907 - val_loss: 0.3896\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3894 - val_loss: 0.3907\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3887 - val_loss: 0.3894\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3876 - val_loss: 0.3869\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3866 - val_loss: 0.3865\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3860 - val_loss: 0.3880\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3851 - val_loss: 0.3851\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3842 - val_loss: 0.3846\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3836 - val_loss: 0.3857\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3827 - val_loss: 0.3875\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3820 - val_loss: 0.3860\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3814 - val_loss: 0.3838\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3803 - val_loss: 0.3844\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3797 - val_loss: 0.3839\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3792 - val_loss: 0.3796\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3785 - val_loss: 0.3800\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3778 - val_loss: 0.3812\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3770 - val_loss: 0.3812\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3763 - val_loss: 0.3833\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3761 - val_loss: 0.3780\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3753 - val_loss: 0.3802\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3745 - val_loss: 0.3791\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3739 - val_loss: 0.3754\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3736 - val_loss: 0.3754\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3728 - val_loss: 0.3751\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3724 - val_loss: 0.3743\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3716 - val_loss: 0.3756\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3711 - val_loss: 0.3770\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3706 - val_loss: 0.3757\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3701 - val_loss: 0.3732\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3697 - val_loss: 0.3735\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3688 - val_loss: 0.3728\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3687 - val_loss: 0.3731\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3681 - val_loss: 0.3723\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3678 - val_loss: 0.3707\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3673 - val_loss: 0.3709\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3667 - val_loss: 0.3701\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3661 - val_loss: 0.3713\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3654 - val_loss: 0.3684\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3653 - val_loss: 0.3665\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3646 - val_loss: 0.3654\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3644 - val_loss: 0.3704\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3642 - val_loss: 0.3699\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3637 - val_loss: 0.3660\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3636 - val_loss: 0.3658\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3633 - val_loss: 0.3668\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3623 - val_loss: 0.3657\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3627 - val_loss: 0.3654\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3620 - val_loss: 0.3654\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3618 - val_loss: 0.3648\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3614 - val_loss: 0.3638\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3613 - val_loss: 0.3660\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3610 - val_loss: 0.3656\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3606 - val_loss: 0.3641\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3602 - val_loss: 0.3639\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3598 - val_loss: 0.3660\n",
            "3870/3870 [==============================] - 0s 37us/step\n",
            "[CV]  learning_rate=0.0015411627181247465, n_hidden=2, n_neurons=14, total= 1.3min\n",
            "[CV] learning_rate=0.0015411627181247465, n_hidden=2, n_neurons=14 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 2.2346 - val_loss: 2.9167\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.9466 - val_loss: 0.7951\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.7681 - val_loss: 0.7261\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.7163 - val_loss: 0.6861\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.6833 - val_loss: 0.6530\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.6571 - val_loss: 0.6208\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.6356 - val_loss: 0.6034\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.6173 - val_loss: 0.5866\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.6010 - val_loss: 0.5706\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.5858 - val_loss: 0.5631\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.5722 - val_loss: 0.5446\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.5595 - val_loss: 0.5283\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.5481 - val_loss: 0.5165\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.5372 - val_loss: 0.5053\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.5285 - val_loss: 0.5007\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.5196 - val_loss: 0.4947\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.5122 - val_loss: 0.4862\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.5046 - val_loss: 0.4774\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4982 - val_loss: 0.4732\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4918 - val_loss: 0.4670\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4862 - val_loss: 0.4593\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4807 - val_loss: 0.4569\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4753 - val_loss: 0.4528\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4705 - val_loss: 0.4458\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4660 - val_loss: 0.4441\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4615 - val_loss: 0.4478\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4576 - val_loss: 0.4355\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4535 - val_loss: 0.4393\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4497 - val_loss: 0.4298\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4463 - val_loss: 0.4299\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.4427 - val_loss: 0.4236\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4395 - val_loss: 0.4255\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4364 - val_loss: 0.4202\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4335 - val_loss: 0.4238\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4312 - val_loss: 0.4122\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4286 - val_loss: 0.4166\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4259 - val_loss: 0.4146\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4234 - val_loss: 0.4155\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4214 - val_loss: 0.4114\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4192 - val_loss: 0.4051\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4171 - val_loss: 0.4035\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4152 - val_loss: 0.4082\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4131 - val_loss: 0.4026\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4113 - val_loss: 0.4012\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4096 - val_loss: 0.3999\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4078 - val_loss: 0.3968\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.4058 - val_loss: 0.3940\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4043 - val_loss: 0.4000\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4029 - val_loss: 0.3908\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4016 - val_loss: 0.3923\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4005 - val_loss: 0.3892\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3989 - val_loss: 0.3993\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3979 - val_loss: 0.3911\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3961 - val_loss: 0.3853\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3953 - val_loss: 0.3902\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3944 - val_loss: 0.3887\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3930 - val_loss: 0.3902\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3921 - val_loss: 0.3893\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3911 - val_loss: 0.3882\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3904 - val_loss: 0.3890\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3896 - val_loss: 0.3782\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3885 - val_loss: 0.3897\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3877 - val_loss: 0.3784\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3868 - val_loss: 0.3759\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3860 - val_loss: 0.3773\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3853 - val_loss: 0.3874\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3847 - val_loss: 0.3761\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3841 - val_loss: 0.3799\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3826 - val_loss: 0.3715\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3825 - val_loss: 0.3748\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3816 - val_loss: 0.3797\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3811 - val_loss: 0.3711\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3804 - val_loss: 0.3879\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3799 - val_loss: 0.3713\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3789 - val_loss: 0.3679\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3786 - val_loss: 0.3697\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3779 - val_loss: 0.3732\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3770 - val_loss: 0.3684\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3768 - val_loss: 0.3670\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3762 - val_loss: 0.3770\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3758 - val_loss: 0.3674\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3748 - val_loss: 0.3719\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3743 - val_loss: 0.3636\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3739 - val_loss: 0.3714\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3739 - val_loss: 0.3650\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3732 - val_loss: 0.3661\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3726 - val_loss: 0.3689\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3723 - val_loss: 0.3678\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3719 - val_loss: 0.3665\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3710 - val_loss: 0.3656\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3709 - val_loss: 0.3621\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3704 - val_loss: 0.3676\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3693 - val_loss: 0.3665\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3694 - val_loss: 0.3603\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3690 - val_loss: 0.3650\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3684 - val_loss: 0.3629\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3681 - val_loss: 0.3604\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3677 - val_loss: 0.3643\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3670 - val_loss: 0.3589\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3668 - val_loss: 0.3654\n",
            "3870/3870 [==============================] - 0s 36us/step\n",
            "[CV]  learning_rate=0.0015411627181247465, n_hidden=2, n_neurons=14, total= 1.2min\n",
            "[CV] learning_rate=0.0015411627181247465, n_hidden=2, n_neurons=14 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 1.6663 - val_loss: 14.2512\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.7867 - val_loss: 10.8723\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.7183 - val_loss: 8.1313\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.6755 - val_loss: 6.4213\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.6420 - val_loss: 5.2461\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.6143 - val_loss: 4.2806\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.5898 - val_loss: 3.4030\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.5681 - val_loss: 2.7215\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.5487 - val_loss: 2.1787\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.5314 - val_loss: 1.7443\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.5169 - val_loss: 1.3417\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.5042 - val_loss: 1.0745\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4933 - val_loss: 0.8555\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4838 - val_loss: 0.6959\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4753 - val_loss: 0.5862\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4678 - val_loss: 0.5170\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4614 - val_loss: 0.4993\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4549 - val_loss: 0.4878\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4497 - val_loss: 0.4815\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4442 - val_loss: 0.4728\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4395 - val_loss: 0.4670\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4355 - val_loss: 0.4613\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4311 - val_loss: 0.4575\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4277 - val_loss: 0.4528\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4244 - val_loss: 0.4494\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4211 - val_loss: 0.4447\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4181 - val_loss: 0.4427\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4157 - val_loss: 0.4402\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4134 - val_loss: 0.4363\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4112 - val_loss: 0.4348\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4089 - val_loss: 0.4325\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4072 - val_loss: 0.4305\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4049 - val_loss: 0.4288\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4040 - val_loss: 0.4277\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4021 - val_loss: 0.4257\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4010 - val_loss: 0.4249\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3995 - val_loss: 0.4230\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3984 - val_loss: 0.4224\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3972 - val_loss: 0.4237\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3963 - val_loss: 0.4218\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3954 - val_loss: 0.4214\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3944 - val_loss: 0.4221\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3930 - val_loss: 0.4204\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3925 - val_loss: 0.4207\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3916 - val_loss: 0.4230\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3914 - val_loss: 0.4243\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3904 - val_loss: 0.4236\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3897 - val_loss: 0.4253\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3885 - val_loss: 0.4272\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3881 - val_loss: 0.4267\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3879 - val_loss: 0.4277\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3870 - val_loss: 0.4285\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3861 - val_loss: 0.4319\n",
            "3870/3870 [==============================] - 0s 40us/step\n",
            "[CV]  learning_rate=0.0015411627181247465, n_hidden=2, n_neurons=14, total=  38.9s\n",
            "[CV] learning_rate=0.001328902721125077, n_hidden=3, n_neurons=55 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 2.0214 - val_loss: 0.8036\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.7308 - val_loss: 0.6465\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.6575 - val_loss: 0.5979\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.6146 - val_loss: 0.5606\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5785 - val_loss: 0.5269\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5481 - val_loss: 0.4996\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5204 - val_loss: 0.4752\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4973 - val_loss: 0.4553\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4779 - val_loss: 0.4384\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4620 - val_loss: 0.4256\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4481 - val_loss: 0.4155\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4377 - val_loss: 0.4111\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4292 - val_loss: 0.4044\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4220 - val_loss: 0.4030\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4158 - val_loss: 0.3931\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4110 - val_loss: 0.3956\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4065 - val_loss: 0.3921\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4027 - val_loss: 0.3903\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3994 - val_loss: 0.3866\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3960 - val_loss: 0.3858\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3933 - val_loss: 0.3825\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3904 - val_loss: 0.3862\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3884 - val_loss: 0.3838\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3857 - val_loss: 0.3800\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3840 - val_loss: 0.3759\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3817 - val_loss: 0.3754\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3805 - val_loss: 0.3759\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3784 - val_loss: 0.3737\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3769 - val_loss: 0.3729\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3756 - val_loss: 0.3701\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3734 - val_loss: 0.3789\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3726 - val_loss: 0.3732\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3712 - val_loss: 0.3694\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3698 - val_loss: 0.3705\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3683 - val_loss: 0.3712\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3671 - val_loss: 0.3665\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3657 - val_loss: 0.3632\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3651 - val_loss: 0.3672\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3638 - val_loss: 0.3621\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3627 - val_loss: 0.3604\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3616 - val_loss: 0.3613\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3609 - val_loss: 0.3613\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3600 - val_loss: 0.3591\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3584 - val_loss: 0.3550\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3580 - val_loss: 0.3606\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3569 - val_loss: 0.3549\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3559 - val_loss: 0.3517\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3552 - val_loss: 0.3531\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3544 - val_loss: 0.3570\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3528 - val_loss: 0.3579\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3527 - val_loss: 0.3511\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3515 - val_loss: 0.3602\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3508 - val_loss: 0.3524\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3495 - val_loss: 0.3517\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3493 - val_loss: 0.3560\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3477 - val_loss: 0.3473\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3478 - val_loss: 0.3558\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3465 - val_loss: 0.3468\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3462 - val_loss: 0.3495\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3447 - val_loss: 0.3481\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3441 - val_loss: 0.3425\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3436 - val_loss: 0.3466\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3426 - val_loss: 0.3410\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3420 - val_loss: 0.3534\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3414 - val_loss: 0.3466\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3405 - val_loss: 0.3436\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3395 - val_loss: 0.3510\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3392 - val_loss: 0.3464\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3384 - val_loss: 0.3454\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3379 - val_loss: 0.3416\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3369 - val_loss: 0.3398\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3358 - val_loss: 0.3430\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3357 - val_loss: 0.3409\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3351 - val_loss: 0.3436\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3339 - val_loss: 0.3358\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3333 - val_loss: 0.3454\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3330 - val_loss: 0.3345\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.3320 - val_loss: 0.3439\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3314 - val_loss: 0.3369\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3309 - val_loss: 0.3376\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3302 - val_loss: 0.3386\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.3293 - val_loss: 0.3391\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3283 - val_loss: 0.3353\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3282 - val_loss: 0.3348\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3270 - val_loss: 0.3342\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3272 - val_loss: 0.3354\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3259 - val_loss: 0.3322\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3254 - val_loss: 0.3330\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3249 - val_loss: 0.3283\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3245 - val_loss: 0.3292\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3237 - val_loss: 0.3300\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3233 - val_loss: 0.3332\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3224 - val_loss: 0.3297\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3216 - val_loss: 0.3326\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3214 - val_loss: 0.3272\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3202 - val_loss: 0.3324\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3199 - val_loss: 0.3237\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3195 - val_loss: 0.3298\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3188 - val_loss: 0.3277\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 112us/step - loss: 0.3184 - val_loss: 0.3206\n",
            "3870/3870 [==============================] - 0s 45us/step\n",
            "[CV]  learning_rate=0.001328902721125077, n_hidden=3, n_neurons=55, total= 1.3min\n",
            "[CV] learning_rate=0.001328902721125077, n_hidden=3, n_neurons=55 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 121us/step - loss: 2.1304 - val_loss: 0.9107\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 113us/step - loss: 0.7822 - val_loss: 0.7231\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 115us/step - loss: 0.6853 - val_loss: 0.6447\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 112us/step - loss: 0.6282 - val_loss: 0.5945\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.5843 - val_loss: 0.5476\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5484 - val_loss: 0.5157\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.5193 - val_loss: 0.5411\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4956 - val_loss: 0.4634\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4776 - val_loss: 0.4965\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4611 - val_loss: 0.4338\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4510 - val_loss: 0.4734\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4390 - val_loss: 0.4170\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4304 - val_loss: 0.4639\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.4219 - val_loss: 0.4036\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4152 - val_loss: 0.4032\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4091 - val_loss: 0.4135\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4040 - val_loss: 0.3991\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3989 - val_loss: 0.4047\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3950 - val_loss: 0.4024\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3910 - val_loss: 0.3771\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3880 - val_loss: 0.3852\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3843 - val_loss: 0.3743\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3814 - val_loss: 0.3911\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3787 - val_loss: 0.3793\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3759 - val_loss: 0.3697\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3736 - val_loss: 0.3851\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3716 - val_loss: 0.3610\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3698 - val_loss: 0.3715\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3675 - val_loss: 0.3656\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3657 - val_loss: 0.3719\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 110us/step - loss: 0.3638 - val_loss: 0.3607\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3624 - val_loss: 0.3690\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3605 - val_loss: 0.3557\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3589 - val_loss: 0.3668\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3578 - val_loss: 0.3517\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3561 - val_loss: 0.3656\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3548 - val_loss: 0.3511\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3529 - val_loss: 0.3618\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3522 - val_loss: 0.3500\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3505 - val_loss: 0.3538\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3494 - val_loss: 0.3451\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3490 - val_loss: 0.3543\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3476 - val_loss: 0.3522\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3462 - val_loss: 0.3467\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3455 - val_loss: 0.3651\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3446 - val_loss: 0.3387\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3435 - val_loss: 0.3711\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3425 - val_loss: 0.3362\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3416 - val_loss: 0.3511\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3404 - val_loss: 0.3359\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3393 - val_loss: 0.3454\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3385 - val_loss: 0.3441\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3377 - val_loss: 0.3433\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3364 - val_loss: 0.3316\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3359 - val_loss: 0.3529\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3351 - val_loss: 0.3303\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3341 - val_loss: 0.3493\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3333 - val_loss: 0.3317\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3326 - val_loss: 0.3593\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3326 - val_loss: 0.3268\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3306 - val_loss: 0.3556\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3298 - val_loss: 0.3317\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3293 - val_loss: 0.3529\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3287 - val_loss: 0.3268\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3280 - val_loss: 0.3591\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3282 - val_loss: 0.3257\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 118us/step - loss: 0.3274 - val_loss: 0.3383\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 119us/step - loss: 0.3256 - val_loss: 0.3330\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 112us/step - loss: 0.3249 - val_loss: 0.3235\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 117us/step - loss: 0.3240 - val_loss: 0.3476\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 115us/step - loss: 0.3231 - val_loss: 0.3226\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 109us/step - loss: 0.3231 - val_loss: 0.3321\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 112us/step - loss: 0.3214 - val_loss: 0.3229\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 115us/step - loss: 0.3207 - val_loss: 0.3286\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 113us/step - loss: 0.3206 - val_loss: 0.3291\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 113us/step - loss: 0.3200 - val_loss: 0.3266\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 112us/step - loss: 0.3192 - val_loss: 0.3281\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 113us/step - loss: 0.3183 - val_loss: 0.3565\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3197 - val_loss: 0.3170\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3182 - val_loss: 0.3377\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3168 - val_loss: 0.3169\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3158 - val_loss: 0.3448\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3155 - val_loss: 0.3161\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3140 - val_loss: 0.3325\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3144 - val_loss: 0.3153\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3131 - val_loss: 0.3190\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3125 - val_loss: 0.3269\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3119 - val_loss: 0.3147\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3111 - val_loss: 0.3175\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3103 - val_loss: 0.3148\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3106 - val_loss: 0.3240\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3096 - val_loss: 0.3133\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.3095 - val_loss: 0.3223\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3087 - val_loss: 0.3109\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3085 - val_loss: 0.3126\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3088 - val_loss: 0.3451\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3084 - val_loss: 0.3118\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3067 - val_loss: 0.3420\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3070 - val_loss: 0.3135\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3055 - val_loss: 0.3167\n",
            "3870/3870 [==============================] - 0s 38us/step\n",
            "[CV]  learning_rate=0.001328902721125077, n_hidden=3, n_neurons=55, total= 1.4min\n",
            "[CV] learning_rate=0.001328902721125077, n_hidden=3, n_neurons=55 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 1.8934 - val_loss: 3.4762\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.7964 - val_loss: 2.2206\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.6576 - val_loss: 1.4185\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.6007 - val_loss: 0.9545\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5658 - val_loss: 0.7135\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.5401 - val_loss: 0.5821\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5193 - val_loss: 0.5136\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.5030 - val_loss: 0.4991\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4885 - val_loss: 0.5116\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.4766 - val_loss: 0.5253\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4664 - val_loss: 0.5412\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4570 - val_loss: 0.5390\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4491 - val_loss: 0.5448\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4410 - val_loss: 0.5461\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4347 - val_loss: 0.5506\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4282 - val_loss: 0.5413\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4220 - val_loss: 0.5413\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4167 - val_loss: 0.5350\n",
            "3870/3870 [==============================] - 0s 39us/step\n",
            "[CV]  learning_rate=0.001328902721125077, n_hidden=3, n_neurons=55, total=  14.6s\n",
            "[CV] learning_rate=0.011366299432595662, n_hidden=2, n_neurons=79 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.8311 - val_loss: 3.9213\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.6058 - val_loss: 1.7414\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4904 - val_loss: 0.4424\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4370 - val_loss: 0.4125\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4160 - val_loss: 0.4207\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4032 - val_loss: 0.3826\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3943 - val_loss: 0.3595\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3840 - val_loss: 0.4033\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3836 - val_loss: 0.3453\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3724 - val_loss: 0.4009\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3760 - val_loss: 0.3385\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3588 - val_loss: 0.3563\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3544 - val_loss: 0.3461\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3511 - val_loss: 0.3339\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3479 - val_loss: 0.3281\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3454 - val_loss: 0.3373\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3412 - val_loss: 0.4619\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3511 - val_loss: 0.3251\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3403 - val_loss: 0.3875\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3319 - val_loss: 0.3143\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3318 - val_loss: 0.3803\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3274 - val_loss: 0.3230\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3230 - val_loss: 0.3169\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3235 - val_loss: 0.3411\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3210 - val_loss: 0.3179\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3220 - val_loss: 0.3714\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3153 - val_loss: 0.3100\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3129 - val_loss: 0.3277\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3106 - val_loss: 0.3092\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3108 - val_loss: 0.3554\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3136 - val_loss: 0.2989\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3073 - val_loss: 0.3587\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3077 - val_loss: 0.3063\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3032 - val_loss: 0.3064\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3029 - val_loss: 0.3608\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3029 - val_loss: 0.3061\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.2990 - val_loss: 0.3290\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3005 - val_loss: 0.2993\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.2972 - val_loss: 0.3115\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.2957 - val_loss: 0.3108\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.2948 - val_loss: 0.3385\n",
            "3870/3870 [==============================] - 0s 38us/step\n",
            "[CV]  learning_rate=0.011366299432595662, n_hidden=2, n_neurons=79, total=  30.3s\n",
            "[CV] learning_rate=0.011366299432595662, n_hidden=2, n_neurons=79 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.9034 - val_loss: 0.8246\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4839 - val_loss: 0.4405\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4287 - val_loss: 0.4672\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4053 - val_loss: 0.3923\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3937 - val_loss: 0.4655\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3855 - val_loss: 0.3683\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3807 - val_loss: 0.4109\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3727 - val_loss: 0.3615\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3653 - val_loss: 0.3904\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3633 - val_loss: 0.3615\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3591 - val_loss: 0.3713\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3537 - val_loss: 0.3732\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3520 - val_loss: 0.3469\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3494 - val_loss: 0.3715\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3462 - val_loss: 0.3424\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3416 - val_loss: 0.3524\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3396 - val_loss: 0.3457\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3373 - val_loss: 0.3366\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3367 - val_loss: 0.3857\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3330 - val_loss: 0.3261\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3350 - val_loss: 0.4023\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3301 - val_loss: 0.3306\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3267 - val_loss: 0.3328\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3234 - val_loss: 0.3370\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3229 - val_loss: 0.3182\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3199 - val_loss: 0.3193\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3178 - val_loss: 0.3234\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3171 - val_loss: 0.3258\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3193 - val_loss: 0.3642\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3145 - val_loss: 0.3123\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3117 - val_loss: 0.3467\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3086 - val_loss: 0.3076\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3076 - val_loss: 0.3148\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3065 - val_loss: 0.3550\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3052 - val_loss: 0.3118\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3034 - val_loss: 0.3258\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3009 - val_loss: 0.3044\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.2996 - val_loss: 0.3305\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.2988 - val_loss: 0.3080\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.2980 - val_loss: 0.3165\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.2951 - val_loss: 0.3270\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.2934 - val_loss: 0.3132\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.2923 - val_loss: 0.3133\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.2908 - val_loss: 0.3521\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.2924 - val_loss: 0.3275\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.2899 - val_loss: 0.3085\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.2894 - val_loss: 0.3339\n",
            "3870/3870 [==============================] - 0s 41us/step\n",
            "[CV]  learning_rate=0.011366299432595662, n_hidden=2, n_neurons=79, total=  34.8s\n",
            "[CV] learning_rate=0.011366299432595662, n_hidden=2, n_neurons=79 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.8038 - val_loss: 0.5815\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4796 - val_loss: 0.5588\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4252 - val_loss: 0.5106\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4285 - val_loss: 0.4415\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3875 - val_loss: 0.3770\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3777 - val_loss: 0.3888\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3799 - val_loss: 0.4155\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3627 - val_loss: 0.4139\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3531 - val_loss: 0.5013\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3474 - val_loss: 0.5261\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3488 - val_loss: 0.5198\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3375 - val_loss: 0.5254\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3326 - val_loss: 0.5262\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3305 - val_loss: 0.4783\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3275 - val_loss: 0.5919\n",
            "3870/3870 [==============================] - 0s 40us/step\n",
            "[CV]  learning_rate=0.011366299432595662, n_hidden=2, n_neurons=79, total=  11.6s\n",
            "[CV] learning_rate=0.023700228460496263, n_hidden=2, n_neurons=22 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.7184 - val_loss: 0.9590\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4875 - val_loss: 1.7677\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.8496 - val_loss: 4.7435\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 3.3398 - val_loss: 1.0851\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5091 - val_loss: 0.5400\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4764 - val_loss: 0.3736\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4297 - val_loss: 0.3800\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4161 - val_loss: 0.3625\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.4178 - val_loss: 0.3574\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4043 - val_loss: 0.3547\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4065 - val_loss: 0.3475\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3999 - val_loss: 0.3510\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3931 - val_loss: 0.3584\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3867 - val_loss: 0.4319\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3840 - val_loss: 0.3664\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3794 - val_loss: 0.3451\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3774 - val_loss: 0.3451\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3715 - val_loss: 0.3640\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3731 - val_loss: 0.3427\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3737 - val_loss: 0.3621\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3695 - val_loss: 0.3501\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3664 - val_loss: 0.3294\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3656 - val_loss: 0.3257\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3595 - val_loss: 0.3416\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3549 - val_loss: 0.3369\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3542 - val_loss: 0.3299\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3539 - val_loss: 0.3519\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3511 - val_loss: 0.3432\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3470 - val_loss: 0.3281\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3500 - val_loss: 0.3294\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3474 - val_loss: 0.3208\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3445 - val_loss: 0.3251\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3479 - val_loss: 0.3430\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3412 - val_loss: 0.3188\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3454 - val_loss: 0.3202\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3399 - val_loss: 0.3266\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3413 - val_loss: 0.3261\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3407 - val_loss: 0.3216\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3349 - val_loss: 0.3377\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3340 - val_loss: 0.3289\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3362 - val_loss: 0.3285\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3324 - val_loss: 0.3086\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3289 - val_loss: 0.3271\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3316 - val_loss: 0.3214\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3307 - val_loss: 0.3097\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3314 - val_loss: 0.3126\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3272 - val_loss: 0.3243\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3286 - val_loss: 0.3135\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3270 - val_loss: 0.3163\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3266 - val_loss: 0.3074\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3279 - val_loss: 0.3204\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3230 - val_loss: 0.3273\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3262 - val_loss: 0.3195\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3243 - val_loss: 0.3122\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3237 - val_loss: 0.3197\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3240 - val_loss: 0.3173\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3243 - val_loss: 0.3255\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3235 - val_loss: 0.3246\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3219 - val_loss: 0.3219\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3223 - val_loss: 0.3176\n",
            "3870/3870 [==============================] - 0s 40us/step\n",
            "[CV]  learning_rate=0.023700228460496263, n_hidden=2, n_neurons=22, total=  44.5s\n",
            "[CV] learning_rate=0.023700228460496263, n_hidden=2, n_neurons=22 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.7685 - val_loss: 0.5308\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4913 - val_loss: 0.6790\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4323 - val_loss: 0.4011\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3997 - val_loss: 0.4502\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3808 - val_loss: 0.3618\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3728 - val_loss: 0.3828\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3615 - val_loss: 0.3516\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3521 - val_loss: 0.4050\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3462 - val_loss: 0.3298\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3430 - val_loss: 0.3302\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3413 - val_loss: 0.3405\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3323 - val_loss: 0.3231\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3344 - val_loss: 0.3367\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3276 - val_loss: 0.3450\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3289 - val_loss: 0.3280\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3269 - val_loss: 0.3683\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3188 - val_loss: 0.3309\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3188 - val_loss: 0.3256\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3178 - val_loss: 0.3158\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3135 - val_loss: 0.3135\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3140 - val_loss: 0.3426\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3119 - val_loss: 0.3285\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3099 - val_loss: 0.3088\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3064 - val_loss: 0.3247\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3102 - val_loss: 0.3148\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3082 - val_loss: 0.3142\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3044 - val_loss: 0.3280\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3030 - val_loss: 0.3225\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3016 - val_loss: 0.3698\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3052 - val_loss: 0.3169\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.2972 - val_loss: 0.3350\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.2986 - val_loss: 0.3397\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.2987 - val_loss: 0.3406\n",
            "3870/3870 [==============================] - 0s 36us/step\n",
            "[CV]  learning_rate=0.023700228460496263, n_hidden=2, n_neurons=22, total=  24.3s\n",
            "[CV] learning_rate=0.023700228460496263, n_hidden=2, n_neurons=22 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.6675 - val_loss: 0.4391\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4216 - val_loss: 0.7288\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4030 - val_loss: 0.3920\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4200 - val_loss: 0.4743\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3684 - val_loss: 0.3936\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3621 - val_loss: 0.9500\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3718 - val_loss: 0.3454\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3454 - val_loss: 0.3334\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3423 - val_loss: 0.6340\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3357 - val_loss: 0.4569\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3529 - val_loss: 0.3797\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3364 - val_loss: 0.3844\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3258 - val_loss: 0.3460\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3233 - val_loss: 0.3956\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3221 - val_loss: 0.3865\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3188 - val_loss: 0.7790\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3194 - val_loss: 0.3109\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3202 - val_loss: 0.3406\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3177 - val_loss: 0.6370\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.3127 - val_loss: 0.8207\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3095 - val_loss: 0.6602\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3140 - val_loss: 0.6782\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3103 - val_loss: 0.8158\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3061 - val_loss: 0.3599\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3062 - val_loss: 0.5569\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.3049 - val_loss: 0.5129\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3025 - val_loss: 0.6427\n",
            "3870/3870 [==============================] - 0s 37us/step\n",
            "[CV]  learning_rate=0.023700228460496263, n_hidden=2, n_neurons=22, total=  20.4s\n",
            "[CV] learning_rate=0.0009885704270178903, n_hidden=1, n_neurons=85 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 2.2519 - val_loss: 1.2811\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.8748 - val_loss: 0.7530\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.7547 - val_loss: 0.7038\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.7156 - val_loss: 0.6600\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.6860 - val_loss: 0.6451\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.6602 - val_loss: 0.6212\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.6374 - val_loss: 0.5926\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.6172 - val_loss: 0.5756\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5988 - val_loss: 0.5652\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.5826 - val_loss: 0.5427\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5683 - val_loss: 0.5330\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5552 - val_loss: 0.5190\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.5438 - val_loss: 0.5077\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.5337 - val_loss: 0.5031\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5245 - val_loss: 0.4900\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5162 - val_loss: 0.4814\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5087 - val_loss: 0.4841\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5023 - val_loss: 0.4732\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4965 - val_loss: 0.4718\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4911 - val_loss: 0.4681\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4865 - val_loss: 0.4626\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4820 - val_loss: 0.4544\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4781 - val_loss: 0.4528\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4743 - val_loss: 0.4468\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4709 - val_loss: 0.4490\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.4677 - val_loss: 0.4435\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4647 - val_loss: 0.4390\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4620 - val_loss: 0.4391\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4595 - val_loss: 0.4404\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4571 - val_loss: 0.4359\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4546 - val_loss: 0.4327\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4522 - val_loss: 0.4358\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4503 - val_loss: 0.4306\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4482 - val_loss: 0.4292\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4462 - val_loss: 0.4263\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4442 - val_loss: 0.4239\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4425 - val_loss: 0.4235\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4408 - val_loss: 0.4177\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4391 - val_loss: 0.4172\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4375 - val_loss: 0.4152\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4361 - val_loss: 0.4150\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4343 - val_loss: 0.4170\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4329 - val_loss: 0.4124\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4316 - val_loss: 0.4102\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4302 - val_loss: 0.4102\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4288 - val_loss: 0.4095\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4274 - val_loss: 0.4046\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4263 - val_loss: 0.4043\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4251 - val_loss: 0.4073\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4239 - val_loss: 0.4019\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4228 - val_loss: 0.4038\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4217 - val_loss: 0.4012\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4206 - val_loss: 0.3987\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4195 - val_loss: 0.4008\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4183 - val_loss: 0.3970\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4174 - val_loss: 0.3981\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4164 - val_loss: 0.3940\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4155 - val_loss: 0.3972\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4144 - val_loss: 0.3965\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4135 - val_loss: 0.3932\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4126 - val_loss: 0.3942\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4117 - val_loss: 0.3933\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4108 - val_loss: 0.3935\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4098 - val_loss: 0.3861\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4089 - val_loss: 0.3888\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4082 - val_loss: 0.3901\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4073 - val_loss: 0.3929\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4066 - val_loss: 0.3853\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4059 - val_loss: 0.3889\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4051 - val_loss: 0.3849\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4044 - val_loss: 0.3870\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4037 - val_loss: 0.3872\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4029 - val_loss: 0.3819\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4024 - val_loss: 0.3862\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4016 - val_loss: 0.3826\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4011 - val_loss: 0.3884\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4005 - val_loss: 0.3829\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3999 - val_loss: 0.3865\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3992 - val_loss: 0.3811\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3987 - val_loss: 0.3846\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3981 - val_loss: 0.3828\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3976 - val_loss: 0.3811\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3969 - val_loss: 0.3804\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3964 - val_loss: 0.3809\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3960 - val_loss: 0.3792\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3954 - val_loss: 0.3797\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3948 - val_loss: 0.3786\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3944 - val_loss: 0.3771\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.3939 - val_loss: 0.3757\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3934 - val_loss: 0.3761\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3930 - val_loss: 0.3771\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3926 - val_loss: 0.3761\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3921 - val_loss: 0.3787\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3917 - val_loss: 0.3738\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3911 - val_loss: 0.3774\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3908 - val_loss: 0.3739\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3902 - val_loss: 0.3772\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3899 - val_loss: 0.3741\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.3895 - val_loss: 0.3712\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3892 - val_loss: 0.3757\n",
            "3870/3870 [==============================] - 0s 39us/step\n",
            "[CV]  learning_rate=0.0009885704270178903, n_hidden=1, n_neurons=85, total= 1.2min\n",
            "[CV] learning_rate=0.0009885704270178903, n_hidden=1, n_neurons=85 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 2.2910 - val_loss: 1.3147\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.8964 - val_loss: 0.7911\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.7532 - val_loss: 0.7130\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.7131 - val_loss: 0.6909\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.6862 - val_loss: 0.6538\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.6650 - val_loss: 0.6350\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.6452 - val_loss: 0.6147\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.6275 - val_loss: 0.5948\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.6122 - val_loss: 0.5797\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5978 - val_loss: 0.5760\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.5843 - val_loss: 0.5554\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5722 - val_loss: 0.5451\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5609 - val_loss: 0.5401\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5507 - val_loss: 0.5197\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.5413 - val_loss: 0.5112\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.5327 - val_loss: 0.5061\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5252 - val_loss: 0.5015\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5179 - val_loss: 0.4901\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.5114 - val_loss: 0.4834\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5052 - val_loss: 0.4750\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5000 - val_loss: 0.4728\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.4946 - val_loss: 0.4688\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4898 - val_loss: 0.4596\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.4855 - val_loss: 0.4633\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4813 - val_loss: 0.4531\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.4774 - val_loss: 0.4548\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4737 - val_loss: 0.4504\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4699 - val_loss: 0.4408\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4670 - val_loss: 0.4392\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4637 - val_loss: 0.4362\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4610 - val_loss: 0.4342\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4580 - val_loss: 0.4354\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4551 - val_loss: 0.4321\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4527 - val_loss: 0.4261\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4502 - val_loss: 0.4235\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4481 - val_loss: 0.4257\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4457 - val_loss: 0.4196\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4437 - val_loss: 0.4175\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.4414 - val_loss: 0.4146\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4396 - val_loss: 0.4127\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4378 - val_loss: 0.4160\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.4358 - val_loss: 0.4098\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4343 - val_loss: 0.4155\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4324 - val_loss: 0.4106\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4309 - val_loss: 0.4104\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4292 - val_loss: 0.4054\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4279 - val_loss: 0.4037\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4263 - val_loss: 0.4067\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4250 - val_loss: 0.4028\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4235 - val_loss: 0.3998\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4223 - val_loss: 0.3999\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4211 - val_loss: 0.3996\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4197 - val_loss: 0.3977\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4187 - val_loss: 0.3975\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4174 - val_loss: 0.3972\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4164 - val_loss: 0.3969\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.4152 - val_loss: 0.3955\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4140 - val_loss: 0.3928\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.4131 - val_loss: 0.3917\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4120 - val_loss: 0.3906\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4112 - val_loss: 0.3939\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4101 - val_loss: 0.3917\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4091 - val_loss: 0.3895\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4082 - val_loss: 0.3893\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4072 - val_loss: 0.3884\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4064 - val_loss: 0.3874\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4056 - val_loss: 0.3854\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4047 - val_loss: 0.3894\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4038 - val_loss: 0.3865\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4030 - val_loss: 0.3846\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4021 - val_loss: 0.3839\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4014 - val_loss: 0.3831\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4006 - val_loss: 0.3823\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3998 - val_loss: 0.3832\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3991 - val_loss: 0.3818\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3982 - val_loss: 0.3811\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3976 - val_loss: 0.3789\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3968 - val_loss: 0.3813\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3962 - val_loss: 0.3789\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3955 - val_loss: 0.3770\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3946 - val_loss: 0.3780\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3939 - val_loss: 0.3770\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3934 - val_loss: 0.3769\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3926 - val_loss: 0.3761\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3920 - val_loss: 0.3760\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3913 - val_loss: 0.3777\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3908 - val_loss: 0.3758\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.3900 - val_loss: 0.3760\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3894 - val_loss: 0.3742\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3889 - val_loss: 0.3743\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.3882 - val_loss: 0.3732\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3878 - val_loss: 0.3747\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3872 - val_loss: 0.3724\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.3866 - val_loss: 0.3751\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3860 - val_loss: 0.3740\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3855 - val_loss: 0.3724\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3851 - val_loss: 0.3736\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3845 - val_loss: 0.3731\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3840 - val_loss: 0.3696\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3835 - val_loss: 0.3701\n",
            "3870/3870 [==============================] - 0s 35us/step\n",
            "[CV]  learning_rate=0.0009885704270178903, n_hidden=1, n_neurons=85, total= 1.2min\n",
            "[CV] learning_rate=0.0009885704270178903, n_hidden=1, n_neurons=85 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 2.7394 - val_loss: 2.2230\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.8745 - val_loss: 1.8715\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.6957 - val_loss: 1.5690\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.6503 - val_loss: 1.3077\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.6227 - val_loss: 1.0925\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.6004 - val_loss: 0.9324\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5816 - val_loss: 0.7968\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5649 - val_loss: 0.7049\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.5504 - val_loss: 0.6247\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5380 - val_loss: 0.5722\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5268 - val_loss: 0.5334\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5168 - val_loss: 0.5090\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5082 - val_loss: 0.4947\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5000 - val_loss: 0.4901\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.4933 - val_loss: 0.4926\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4871 - val_loss: 0.5001\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4814 - val_loss: 0.5098\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4765 - val_loss: 0.5246\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4715 - val_loss: 0.5448\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4676 - val_loss: 0.5590\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4637 - val_loss: 0.5756\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4601 - val_loss: 0.5947\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4568 - val_loss: 0.6091\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4539 - val_loss: 0.6293\n",
            "3870/3870 [==============================] - 0s 38us/step\n",
            "[CV]  learning_rate=0.0009885704270178903, n_hidden=1, n_neurons=85, total=  16.6s\n",
            "[CV] learning_rate=0.0008947114991152286, n_hidden=0, n_neurons=75 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 6.4285 - val_loss: 4.4589\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 2.3187 - val_loss: 1.7061\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 1.2266 - val_loss: 0.9640\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.8491 - val_loss: 0.7181\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.7002 - val_loss: 0.6229\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.6379 - val_loss: 0.5840\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.6101 - val_loss: 0.5659\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5967 - val_loss: 0.5566\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5892 - val_loss: 0.5514\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5847 - val_loss: 0.5477\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5813 - val_loss: 0.5449\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5784 - val_loss: 0.5422\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 77us/step - loss: 0.5760 - val_loss: 0.5400\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.5737 - val_loss: 0.5385\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5720 - val_loss: 0.5357\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5700 - val_loss: 0.5337\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.5681 - val_loss: 0.5319\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5664 - val_loss: 0.5304\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5652 - val_loss: 0.5286\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5637 - val_loss: 0.5271\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.5622 - val_loss: 0.5259\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5612 - val_loss: 0.5244\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5600 - val_loss: 0.5232\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5587 - val_loss: 0.5223\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5579 - val_loss: 0.5210\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5569 - val_loss: 0.5204\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5562 - val_loss: 0.5191\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5553 - val_loss: 0.5182\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5543 - val_loss: 0.5178\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5540 - val_loss: 0.5167\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5531 - val_loss: 0.5160\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5526 - val_loss: 0.5155\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5518 - val_loss: 0.5154\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5514 - val_loss: 0.5152\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5509 - val_loss: 0.5145\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5506 - val_loss: 0.5140\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5505 - val_loss: 0.5126\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5497 - val_loss: 0.5122\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5493 - val_loss: 0.5117\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5490 - val_loss: 0.5115\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5485 - val_loss: 0.5111\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5484 - val_loss: 0.5107\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.5480 - val_loss: 0.5105\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5478 - val_loss: 0.5102\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5474 - val_loss: 0.5100\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5473 - val_loss: 0.5095\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5470 - val_loss: 0.5092\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5469 - val_loss: 0.5091\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5465 - val_loss: 0.5089\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5463 - val_loss: 0.5087\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5464 - val_loss: 0.5083\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.5460 - val_loss: 0.5084\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5461 - val_loss: 0.5084\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5460 - val_loss: 0.5079\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5458 - val_loss: 0.5079\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5455 - val_loss: 0.5076\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5456 - val_loss: 0.5075\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5454 - val_loss: 0.5076\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5452 - val_loss: 0.5073\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5450 - val_loss: 0.5075\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5453 - val_loss: 0.5072\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5451 - val_loss: 0.5071\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5449 - val_loss: 0.5075\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5450 - val_loss: 0.5071\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5448 - val_loss: 0.5071\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5450 - val_loss: 0.5067\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5447 - val_loss: 0.5068\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5448 - val_loss: 0.5067\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5448 - val_loss: 0.5067\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5446 - val_loss: 0.5064\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5445 - val_loss: 0.5063\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5445 - val_loss: 0.5065\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5445 - val_loss: 0.5063\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5445 - val_loss: 0.5062\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5445 - val_loss: 0.5062\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5444 - val_loss: 0.5061\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5444 - val_loss: 0.5061\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5444 - val_loss: 0.5062\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5444 - val_loss: 0.5063\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5442 - val_loss: 0.5062\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5443 - val_loss: 0.5060\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5441 - val_loss: 0.5064\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5442 - val_loss: 0.5062\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5443 - val_loss: 0.5058\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5442 - val_loss: 0.5057\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5443 - val_loss: 0.5058\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5441 - val_loss: 0.5058\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5440 - val_loss: 0.5058\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5442 - val_loss: 0.5059\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5440 - val_loss: 0.5058\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5440 - val_loss: 0.5056\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5442 - val_loss: 0.5058\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5438 - val_loss: 0.5058\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5441 - val_loss: 0.5056\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5440 - val_loss: 0.5054\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5440 - val_loss: 0.5054\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5441 - val_loss: 0.5055\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5441 - val_loss: 0.5054\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5441 - val_loss: 0.5057\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5441 - val_loss: 0.5058\n",
            "3870/3870 [==============================] - 0s 34us/step\n",
            "[CV]  learning_rate=0.0008947114991152286, n_hidden=0, n_neurons=75, total= 1.1min\n",
            "[CV] learning_rate=0.0008947114991152286, n_hidden=0, n_neurons=75 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 5.0557 - val_loss: 3.9861\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 2.3333 - val_loss: 1.7572\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 1.2698 - val_loss: 0.9934\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.8414 - val_loss: 0.7075\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.6660 - val_loss: 0.5930\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5937 - val_loss: 0.5465\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5635 - val_loss: 0.5271\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5510 - val_loss: 0.5188\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5456 - val_loss: 0.5153\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5432 - val_loss: 0.5136\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5419 - val_loss: 0.5130\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5416 - val_loss: 0.5123\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5412 - val_loss: 0.5119\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5409 - val_loss: 0.5119\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5406 - val_loss: 0.5114\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5404 - val_loss: 0.5110\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5404 - val_loss: 0.5109\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5401 - val_loss: 0.5106\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5401 - val_loss: 0.5104\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5400 - val_loss: 0.5103\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5397 - val_loss: 0.5102\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5398 - val_loss: 0.5099\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5396 - val_loss: 0.5097\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5395 - val_loss: 0.5098\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5394 - val_loss: 0.5098\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5396 - val_loss: 0.5092\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5394 - val_loss: 0.5092\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5393 - val_loss: 0.5093\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5392 - val_loss: 0.5094\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5389 - val_loss: 0.5089\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5390 - val_loss: 0.5087\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5391 - val_loss: 0.5085\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5389 - val_loss: 0.5085\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5389 - val_loss: 0.5083\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5389 - val_loss: 0.5088\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5388 - val_loss: 0.5084\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5387 - val_loss: 0.5082\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5387 - val_loss: 0.5082\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5387 - val_loss: 0.5080\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5387 - val_loss: 0.5083\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5387 - val_loss: 0.5081\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5386 - val_loss: 0.5082\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5387 - val_loss: 0.5080\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5387 - val_loss: 0.5081\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5386 - val_loss: 0.5083\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5384 - val_loss: 0.5077\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5385 - val_loss: 0.5077\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5385 - val_loss: 0.5079\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5383 - val_loss: 0.5075\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5384 - val_loss: 0.5075\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5384 - val_loss: 0.5076\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5386 - val_loss: 0.5075\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5385 - val_loss: 0.5075\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5384 - val_loss: 0.5074\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5385 - val_loss: 0.5074\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5383 - val_loss: 0.5072\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5383 - val_loss: 0.5075\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5385 - val_loss: 0.5073\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5384 - val_loss: 0.5074\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5383 - val_loss: 0.5073\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5384 - val_loss: 0.5072\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5384 - val_loss: 0.5073\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5384 - val_loss: 0.5073\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5384 - val_loss: 0.5073\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5384 - val_loss: 0.5075\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5383 - val_loss: 0.5071\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5383 - val_loss: 0.5070\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5384 - val_loss: 0.5071\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5384 - val_loss: 0.5073\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5383 - val_loss: 0.5071\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5384 - val_loss: 0.5074\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5384 - val_loss: 0.5075\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5383 - val_loss: 0.5074\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5382 - val_loss: 0.5071\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5384 - val_loss: 0.5072\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5383 - val_loss: 0.5070\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5383 - val_loss: 0.5070\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 77us/step - loss: 0.5383 - val_loss: 0.5069\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5383 - val_loss: 0.5070\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5384 - val_loss: 0.5070\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5382 - val_loss: 0.5069\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5383 - val_loss: 0.5069\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5383 - val_loss: 0.5069\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5383 - val_loss: 0.5072\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5384 - val_loss: 0.5070\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.5384 - val_loss: 0.5070\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5383 - val_loss: 0.5070\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5383 - val_loss: 0.5072\n",
            "3870/3870 [==============================] - 0s 37us/step\n",
            "[CV]  learning_rate=0.0008947114991152286, n_hidden=0, n_neurons=75, total=  56.6s\n",
            "[CV] learning_rate=0.0008947114991152286, n_hidden=0, n_neurons=75 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 5.9932 - val_loss: 15.3660\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 2.8812 - val_loss: 13.0426\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 1.5996 - val_loss: 11.9020\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 1.0561 - val_loss: 11.2871\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.8206 - val_loss: 10.9005\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.7146 - val_loss: 10.6198\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.6637 - val_loss: 10.3879\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.6378 - val_loss: 10.1932\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.6228 - val_loss: 10.0139\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.6124 - val_loss: 9.8493\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.6045 - val_loss: 9.6993\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5980 - val_loss: 9.5527\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5920 - val_loss: 9.4148\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5869 - val_loss: 9.2845\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5819 - val_loss: 9.1614\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5772 - val_loss: 9.0397\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5724 - val_loss: 8.9153\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5691 - val_loss: 8.7980\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5653 - val_loss: 8.6870\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5614 - val_loss: 8.5776\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5585 - val_loss: 8.4763\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5552 - val_loss: 8.3725\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5523 - val_loss: 8.2750\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5497 - val_loss: 8.1835\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5470 - val_loss: 8.0874\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5446 - val_loss: 7.9947\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5422 - val_loss: 7.9063\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5400 - val_loss: 7.8212\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5380 - val_loss: 7.7411\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5361 - val_loss: 7.6628\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5339 - val_loss: 7.5829\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5324 - val_loss: 7.5082\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5308 - val_loss: 7.4364\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5293 - val_loss: 7.3663\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5278 - val_loss: 7.2990\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5265 - val_loss: 7.2342\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5252 - val_loss: 7.1650\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5240 - val_loss: 7.0995\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5228 - val_loss: 7.0420\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5216 - val_loss: 6.9831\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5206 - val_loss: 6.9244\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5196 - val_loss: 6.8700\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5187 - val_loss: 6.8144\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5178 - val_loss: 6.7640\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5170 - val_loss: 6.7122\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5161 - val_loss: 6.6614\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5155 - val_loss: 6.6151\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5148 - val_loss: 6.5680\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5141 - val_loss: 6.5233\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5135 - val_loss: 6.4822\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5129 - val_loss: 6.4412\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5124 - val_loss: 6.3967\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 79us/step - loss: 0.5117 - val_loss: 6.3547\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5114 - val_loss: 6.3135\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5109 - val_loss: 6.2794\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5104 - val_loss: 6.2417\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5100 - val_loss: 6.2068\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5096 - val_loss: 6.1724\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5089 - val_loss: 6.1353\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5089 - val_loss: 6.1037\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5085 - val_loss: 6.0724\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5082 - val_loss: 6.0400\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5078 - val_loss: 6.0068\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5076 - val_loss: 5.9794\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5073 - val_loss: 5.9538\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5069 - val_loss: 5.9248\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5067 - val_loss: 5.8969\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5066 - val_loss: 5.8707\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5063 - val_loss: 5.8472\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.5061 - val_loss: 5.8234\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5059 - val_loss: 5.7993\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5056 - val_loss: 5.7775\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5053 - val_loss: 5.7543\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5052 - val_loss: 5.7299\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5051 - val_loss: 5.7108\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5050 - val_loss: 5.6927\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5048 - val_loss: 5.6747\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5046 - val_loss: 5.6522\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5046 - val_loss: 5.6352\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5045 - val_loss: 5.6148\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5042 - val_loss: 5.5971\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5042 - val_loss: 5.5819\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5041 - val_loss: 5.5671\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5039 - val_loss: 5.5536\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5039 - val_loss: 5.5353\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5037 - val_loss: 5.5224\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5036 - val_loss: 5.5074\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5036 - val_loss: 5.4896\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5035 - val_loss: 5.4767\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5035 - val_loss: 5.4629\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5034 - val_loss: 5.4491\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5033 - val_loss: 5.4374\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5033 - val_loss: 5.4244\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5032 - val_loss: 5.4119\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5031 - val_loss: 5.4004\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5030 - val_loss: 5.3892\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5029 - val_loss: 5.3773\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5029 - val_loss: 5.3675\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5030 - val_loss: 5.3593\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5029 - val_loss: 5.3499\n",
            "3870/3870 [==============================] - 0s 36us/step\n",
            "[CV]  learning_rate=0.0008947114991152286, n_hidden=0, n_neurons=75, total= 1.1min\n",
            "[CV] learning_rate=0.0007072925709838409, n_hidden=3, n_neurons=50 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 111us/step - loss: 2.6756 - val_loss: 2.3893\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 1.1387 - val_loss: 1.0603\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.8074 - val_loss: 0.7626\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.7186 - val_loss: 0.6713\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.6804 - val_loss: 0.6355\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.6559 - val_loss: 0.6118\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.6361 - val_loss: 0.5927\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.6181 - val_loss: 0.5752\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 113us/step - loss: 0.6020 - val_loss: 0.5603\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 110us/step - loss: 0.5867 - val_loss: 0.5447\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 114us/step - loss: 0.5724 - val_loss: 0.5310\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 114us/step - loss: 0.5590 - val_loss: 0.5187\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 112us/step - loss: 0.5464 - val_loss: 0.5078\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.5344 - val_loss: 0.4971\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5233 - val_loss: 0.4862\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5128 - val_loss: 0.4796\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.5034 - val_loss: 0.4686\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4944 - val_loss: 0.4603\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4857 - val_loss: 0.4538\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.4781 - val_loss: 0.4456\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4709 - val_loss: 0.4404\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4638 - val_loss: 0.4364\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4579 - val_loss: 0.4282\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4520 - val_loss: 0.4240\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4466 - val_loss: 0.4174\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4417 - val_loss: 0.4145\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4369 - val_loss: 0.4125\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4328 - val_loss: 0.4075\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4290 - val_loss: 0.4056\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.4253 - val_loss: 0.4024\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.4219 - val_loss: 0.4009\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4187 - val_loss: 0.3964\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4156 - val_loss: 0.3949\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4130 - val_loss: 0.3922\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4106 - val_loss: 0.3910\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4083 - val_loss: 0.3879\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4060 - val_loss: 0.3861\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4040 - val_loss: 0.3864\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4024 - val_loss: 0.3842\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4004 - val_loss: 0.3818\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3988 - val_loss: 0.3831\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3972 - val_loss: 0.3791\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3957 - val_loss: 0.3808\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3943 - val_loss: 0.3790\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3930 - val_loss: 0.3773\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3917 - val_loss: 0.3767\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3903 - val_loss: 0.3758\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3893 - val_loss: 0.3748\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3881 - val_loss: 0.3745\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3867 - val_loss: 0.3772\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3861 - val_loss: 0.3732\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3850 - val_loss: 0.3715\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3840 - val_loss: 0.3713\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.3830 - val_loss: 0.3712\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3819 - val_loss: 0.3763\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3812 - val_loss: 0.3752\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3805 - val_loss: 0.3690\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3797 - val_loss: 0.3678\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3788 - val_loss: 0.3680\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3780 - val_loss: 0.3691\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3773 - val_loss: 0.3674\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3762 - val_loss: 0.3676\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3754 - val_loss: 0.3647\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3749 - val_loss: 0.3707\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3744 - val_loss: 0.3647\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3736 - val_loss: 0.3656\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3730 - val_loss: 0.3638\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3724 - val_loss: 0.3636\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3717 - val_loss: 0.3648\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3711 - val_loss: 0.3626\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3704 - val_loss: 0.3605\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 113us/step - loss: 0.3698 - val_loss: 0.3610\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3693 - val_loss: 0.3615\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 109us/step - loss: 0.3683 - val_loss: 0.3614\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3680 - val_loss: 0.3616\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3672 - val_loss: 0.3597\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3668 - val_loss: 0.3566\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3663 - val_loss: 0.3579\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3655 - val_loss: 0.3587\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3651 - val_loss: 0.3579\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3647 - val_loss: 0.3573\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3638 - val_loss: 0.3575\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3635 - val_loss: 0.3583\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3629 - val_loss: 0.3546\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3626 - val_loss: 0.3573\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3618 - val_loss: 0.3560\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3611 - val_loss: 0.3566\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3607 - val_loss: 0.3551\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 1s 110us/step - loss: 0.3600 - val_loss: 0.3557\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 1s 116us/step - loss: 0.3599 - val_loss: 0.3540\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 1s 117us/step - loss: 0.3592 - val_loss: 0.3529\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 1s 117us/step - loss: 0.3588 - val_loss: 0.3522\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 1s 111us/step - loss: 0.3579 - val_loss: 0.3536\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 1s 114us/step - loss: 0.3578 - val_loss: 0.3528\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 1s 114us/step - loss: 0.3569 - val_loss: 0.3561\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 1s 114us/step - loss: 0.3569 - val_loss: 0.3518\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 1s 115us/step - loss: 0.3565 - val_loss: 0.3527\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 1s 113us/step - loss: 0.3560 - val_loss: 0.3490\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 1s 116us/step - loss: 0.3555 - val_loss: 0.3492\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 1s 110us/step - loss: 0.3548 - val_loss: 0.3479\n",
            "3870/3870 [==============================] - 0s 45us/step\n",
            "[CV]  learning_rate=0.0007072925709838409, n_hidden=3, n_neurons=50, total= 1.4min\n",
            "[CV] learning_rate=0.0007072925709838409, n_hidden=3, n_neurons=50 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 115us/step - loss: 2.6772 - val_loss: 2.5531\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 1.2402 - val_loss: 1.1476\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.8767 - val_loss: 0.7965\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.7348 - val_loss: 0.6868\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.6699 - val_loss: 0.6309\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.6354 - val_loss: 0.6033\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.6114 - val_loss: 0.5814\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.5923 - val_loss: 0.5696\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.5744 - val_loss: 0.5555\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.5587 - val_loss: 0.5319\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.5446 - val_loss: 0.5179\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.5311 - val_loss: 0.5110\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5189 - val_loss: 0.4994\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.5068 - val_loss: 0.4785\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4967 - val_loss: 0.4744\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4867 - val_loss: 0.4630\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4773 - val_loss: 0.4549\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 97us/step - loss: 0.4689 - val_loss: 0.4467\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.4611 - val_loss: 0.4381\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4544 - val_loss: 0.4468\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4473 - val_loss: 0.4359\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4419 - val_loss: 0.4316\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4371 - val_loss: 0.4163\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4323 - val_loss: 0.4184\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4282 - val_loss: 0.4151\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4233 - val_loss: 0.4137\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4206 - val_loss: 0.4105\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4170 - val_loss: 0.4075\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4139 - val_loss: 0.4008\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.4109 - val_loss: 0.4008\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4082 - val_loss: 0.4019\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4058 - val_loss: 0.4030\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4035 - val_loss: 0.3969\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4011 - val_loss: 0.4002\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3987 - val_loss: 0.4004\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3970 - val_loss: 0.3947\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3951 - val_loss: 0.3934\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3930 - val_loss: 0.3932\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3916 - val_loss: 0.3928\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.3896 - val_loss: 0.3911\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3884 - val_loss: 0.3906\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3868 - val_loss: 0.3912\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3856 - val_loss: 0.3854\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3843 - val_loss: 0.3872\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3827 - val_loss: 0.3841\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3814 - val_loss: 0.3830\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3806 - val_loss: 0.3822\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3794 - val_loss: 0.3853\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3786 - val_loss: 0.3855\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3774 - val_loss: 0.3874\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.3765 - val_loss: 0.3800\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3754 - val_loss: 0.3827\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3747 - val_loss: 0.3779\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3735 - val_loss: 0.3773\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3727 - val_loss: 0.3771\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3720 - val_loss: 0.3763\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3710 - val_loss: 0.3803\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3699 - val_loss: 0.3778\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3691 - val_loss: 0.3759\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3686 - val_loss: 0.3758\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3676 - val_loss: 0.3728\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3667 - val_loss: 0.3818\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3660 - val_loss: 0.3720\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3655 - val_loss: 0.3790\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3645 - val_loss: 0.3674\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3640 - val_loss: 0.3671\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3627 - val_loss: 0.3752\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3628 - val_loss: 0.3699\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3620 - val_loss: 0.3680\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3610 - val_loss: 0.3727\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3607 - val_loss: 0.3698\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3597 - val_loss: 0.3714\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3593 - val_loss: 0.3693\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3588 - val_loss: 0.3714\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3582 - val_loss: 0.3675\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3576 - val_loss: 0.3722\n",
            "3870/3870 [==============================] - 0s 40us/step\n",
            "[CV]  learning_rate=0.0007072925709838409, n_hidden=3, n_neurons=50, total= 1.0min\n",
            "[CV] learning_rate=0.0007072925709838409, n_hidden=3, n_neurons=50 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 112us/step - loss: 2.7479 - val_loss: 2.5312\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 1.0215 - val_loss: 2.1501\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.8088 - val_loss: 1.8651\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.7422 - val_loss: 1.6644\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.7117 - val_loss: 1.4586\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.6901 - val_loss: 1.3210\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.6718 - val_loss: 1.1938\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.6550 - val_loss: 1.0968\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.6397 - val_loss: 0.9908\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.6246 - val_loss: 0.8973\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.6110 - val_loss: 0.8372\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.5974 - val_loss: 0.7766\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5845 - val_loss: 0.7269\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.5721 - val_loss: 0.6813\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.5600 - val_loss: 0.6411\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.5480 - val_loss: 0.6065\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.5368 - val_loss: 0.5737\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.5257 - val_loss: 0.5500\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.5154 - val_loss: 0.5274\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.5054 - val_loss: 0.5098\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.4962 - val_loss: 0.4961\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4876 - val_loss: 0.4846\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4794 - val_loss: 0.4773\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.4723 - val_loss: 0.4707\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.4654 - val_loss: 0.4669\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4594 - val_loss: 0.4646\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4539 - val_loss: 0.4637\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.4488 - val_loss: 0.4597\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.4440 - val_loss: 0.4585\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4401 - val_loss: 0.4586\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4362 - val_loss: 0.4556\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4327 - val_loss: 0.4520\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.4294 - val_loss: 0.4527\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.4264 - val_loss: 0.4467\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4235 - val_loss: 0.4430\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 107us/step - loss: 0.4210 - val_loss: 0.4434\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.4185 - val_loss: 0.4394\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.4159 - val_loss: 0.4349\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4139 - val_loss: 0.4330\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4118 - val_loss: 0.4283\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.4097 - val_loss: 0.4243\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.4079 - val_loss: 0.4214\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4060 - val_loss: 0.4211\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.4046 - val_loss: 0.4146\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.4024 - val_loss: 0.4113\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4014 - val_loss: 0.4072\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3997 - val_loss: 0.4042\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3982 - val_loss: 0.4029\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3967 - val_loss: 0.3995\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3952 - val_loss: 0.3969\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3938 - val_loss: 0.3938\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 108us/step - loss: 0.3928 - val_loss: 0.3912\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.3915 - val_loss: 0.3893\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3902 - val_loss: 0.3864\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3891 - val_loss: 0.3848\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3877 - val_loss: 0.3834\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3865 - val_loss: 0.3814\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3854 - val_loss: 0.3807\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3844 - val_loss: 0.3794\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3834 - val_loss: 0.3789\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3823 - val_loss: 0.3780\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3812 - val_loss: 0.3777\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3804 - val_loss: 0.3775\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3792 - val_loss: 0.3777\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3784 - val_loss: 0.3789\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.3774 - val_loss: 0.3790\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 0.3764 - val_loss: 0.3789\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 105us/step - loss: 0.3754 - val_loss: 0.3783\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3744 - val_loss: 0.3790\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3737 - val_loss: 0.3807\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 106us/step - loss: 0.3727 - val_loss: 0.3802\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.3718 - val_loss: 0.3870\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 102us/step - loss: 0.3711 - val_loss: 0.3847\n",
            "3870/3870 [==============================] - 0s 38us/step\n",
            "[CV]  learning_rate=0.0007072925709838409, n_hidden=3, n_neurons=50, total=  59.1s\n",
            "[CV] learning_rate=0.008296327112417696, n_hidden=1, n_neurons=92 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.9714 - val_loss: 6.6866\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.7076 - val_loss: 5.0985\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.7877 - val_loss: 0.7216\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.5235 - val_loss: 0.4570\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4582 - val_loss: 0.4192\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4411 - val_loss: 0.4042\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4310 - val_loss: 0.4017\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4230 - val_loss: 0.4013\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4180 - val_loss: 0.3942\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4126 - val_loss: 0.3956\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4078 - val_loss: 0.3875\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4044 - val_loss: 0.3916\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4011 - val_loss: 0.3817\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3985 - val_loss: 0.3804\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3940 - val_loss: 0.3806\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3930 - val_loss: 0.3768\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3888 - val_loss: 0.3701\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3879 - val_loss: 0.3690\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3857 - val_loss: 0.3689\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3885 - val_loss: 0.3650\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3820 - val_loss: 0.3703\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3798 - val_loss: 0.3808\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3802 - val_loss: 0.3717\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4076 - val_loss: 0.3626\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3778 - val_loss: 0.3647\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3754 - val_loss: 0.3580\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3727 - val_loss: 0.3624\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3728 - val_loss: 0.3586\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3693 - val_loss: 0.3581\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3687 - val_loss: 0.3625\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3679 - val_loss: 0.3586\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3659 - val_loss: 0.3541\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3733 - val_loss: 0.3476\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3649 - val_loss: 0.3474\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3642 - val_loss: 0.3514\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3660 - val_loss: 0.3576\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3595 - val_loss: 0.3574\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3607 - val_loss: 0.3472\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3635 - val_loss: 0.3505\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3570 - val_loss: 0.3493\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3548 - val_loss: 0.4784\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3599 - val_loss: 0.3517\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3525 - val_loss: 0.3514\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3562 - val_loss: 0.3495\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3496 - val_loss: 0.3474\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3491 - val_loss: 0.3588\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3524 - val_loss: 0.3765\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3491 - val_loss: 0.3361\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3532 - val_loss: 0.3371\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3468 - val_loss: 0.3508\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3456 - val_loss: 0.3377\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3445 - val_loss: 0.3452\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3455 - val_loss: 0.3340\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3419 - val_loss: 0.3457\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3449 - val_loss: 0.3339\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3405 - val_loss: 0.3387\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3406 - val_loss: 0.3446\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3393 - val_loss: 0.3325\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3394 - val_loss: 0.3270\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3438 - val_loss: 0.3430\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3360 - val_loss: 0.3390\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3352 - val_loss: 0.3383\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3369 - val_loss: 0.3627\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3375 - val_loss: 0.3315\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3359 - val_loss: 0.3340\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3343 - val_loss: 0.3313\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3321 - val_loss: 0.3319\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3335 - val_loss: 0.3400\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3320 - val_loss: 0.3158\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3293 - val_loss: 0.3381\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3333 - val_loss: 0.3251\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3350 - val_loss: 0.3301\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3284 - val_loss: 0.3238\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3318 - val_loss: 0.4112\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3320 - val_loss: 0.3247\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3313 - val_loss: 0.3308\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3249 - val_loss: 0.3920\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3638 - val_loss: 0.3272\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3758 - val_loss: 0.3264\n",
            "3870/3870 [==============================] - 0s 36us/step\n",
            "[CV]  learning_rate=0.008296327112417696, n_hidden=1, n_neurons=92, total=  55.2s\n",
            "[CV] learning_rate=0.008296327112417696, n_hidden=1, n_neurons=92 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 1.7127 - val_loss: 0.7146\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.5572 - val_loss: 0.4872\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4821 - val_loss: 0.4395\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4496 - val_loss: 0.4156\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4309 - val_loss: 0.4077\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.4201 - val_loss: 0.4025\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4127 - val_loss: 0.4019\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4067 - val_loss: 0.3939\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4019 - val_loss: 0.3863\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3965 - val_loss: 0.3848\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3928 - val_loss: 0.3808\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3905 - val_loss: 0.3847\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3864 - val_loss: 0.3770\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.3839 - val_loss: 0.3773\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3822 - val_loss: 0.3727\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3798 - val_loss: 0.3757\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3774 - val_loss: 0.3695\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3762 - val_loss: 0.3718\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3734 - val_loss: 0.3662\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3711 - val_loss: 0.3624\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3696 - val_loss: 0.3740\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3678 - val_loss: 0.3653\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3664 - val_loss: 0.3630\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3643 - val_loss: 0.3672\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3639 - val_loss: 0.3636\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3619 - val_loss: 0.3550\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3609 - val_loss: 0.3577\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3586 - val_loss: 0.3593\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.3575 - val_loss: 0.3522\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3561 - val_loss: 0.3531\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3565 - val_loss: 0.3511\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3553 - val_loss: 0.3510\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3539 - val_loss: 0.3563\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3540 - val_loss: 0.3527\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3520 - val_loss: 0.3546\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3516 - val_loss: 0.3462\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3497 - val_loss: 0.3497\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3502 - val_loss: 0.3520\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3463 - val_loss: 0.3517\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3463 - val_loss: 0.3505\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3458 - val_loss: 0.3475\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3445 - val_loss: 0.3455\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3443 - val_loss: 0.3433\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3437 - val_loss: 0.3421\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3416 - val_loss: 0.3396\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3407 - val_loss: 0.3418\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3399 - val_loss: 0.3463\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3398 - val_loss: 0.3427\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3377 - val_loss: 0.3894\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3387 - val_loss: 0.3441\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3382 - val_loss: 0.3650\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3707 - val_loss: 0.3421\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3352 - val_loss: 0.3373\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3348 - val_loss: 0.3336\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3347 - val_loss: 0.3407\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3329 - val_loss: 0.3417\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3333 - val_loss: 0.3368\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3317 - val_loss: 0.3344\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3305 - val_loss: 0.3389\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3298 - val_loss: 0.3289\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3290 - val_loss: 0.3326\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3280 - val_loss: 0.3276\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3278 - val_loss: 0.3342\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3270 - val_loss: 0.3325\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3243 - val_loss: 0.3303\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3252 - val_loss: 0.3376\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3240 - val_loss: 0.3319\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3237 - val_loss: 0.3446\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3239 - val_loss: 0.3352\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3245 - val_loss: 0.3324\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3230 - val_loss: 0.3172\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3212 - val_loss: 0.3319\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3220 - val_loss: 0.3193\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3206 - val_loss: 0.3266\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3207 - val_loss: 0.3197\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3203 - val_loss: 0.3306\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3191 - val_loss: 0.3300\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3178 - val_loss: 0.3447\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3175 - val_loss: 0.3199\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 1s 100us/step - loss: 0.3158 - val_loss: 0.3280\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 0.3183 - val_loss: 0.3240\n",
            "3870/3870 [==============================] - 0s 50us/step\n",
            "[CV]  learning_rate=0.008296327112417696, n_hidden=1, n_neurons=92, total=  57.0s\n",
            "[CV] learning_rate=0.008296327112417696, n_hidden=1, n_neurons=92 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 104us/step - loss: 0.8705 - val_loss: 0.6272\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.5107 - val_loss: 0.4936\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 101us/step - loss: 0.4664 - val_loss: 0.6026\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 0.4424 - val_loss: 0.6104\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4278 - val_loss: 0.5637\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4189 - val_loss: 0.4810\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4118 - val_loss: 0.4181\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4029 - val_loss: 0.4059\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3975 - val_loss: 0.3848\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3937 - val_loss: 0.3839\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4088 - val_loss: 0.3987\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3853 - val_loss: 0.3977\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3799 - val_loss: 0.4307\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3766 - val_loss: 0.4529\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3799 - val_loss: 0.4829\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3715 - val_loss: 0.4968\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3693 - val_loss: 0.4990\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3720 - val_loss: 0.5754\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3679 - val_loss: 0.5893\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3616 - val_loss: 0.5712\n",
            "3870/3870 [==============================] - 0s 36us/step\n",
            "[CV]  learning_rate=0.008296327112417696, n_hidden=1, n_neurons=92, total=  14.6s\n",
            "[CV] learning_rate=0.0026242047821147525, n_hidden=0, n_neurons=55 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 3.0789 - val_loss: 1.0785\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.7950 - val_loss: 0.6533\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.6116 - val_loss: 0.5608\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5878 - val_loss: 0.5799\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5761 - val_loss: 0.5525\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5785 - val_loss: 0.5421\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5683 - val_loss: 0.5569\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5640 - val_loss: 0.5351\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5596 - val_loss: 0.5243\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5596 - val_loss: 0.5245\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5566 - val_loss: 0.5208\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5544 - val_loss: 0.5218\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5516 - val_loss: 0.5126\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5529 - val_loss: 0.5468\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5503 - val_loss: 0.5116\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5503 - val_loss: 0.5459\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5473 - val_loss: 0.5269\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5503 - val_loss: 0.5094\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5494 - val_loss: 0.5258\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5481 - val_loss: 0.5291\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5469 - val_loss: 0.5075\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5482 - val_loss: 0.5068\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5472 - val_loss: 0.5087\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5465 - val_loss: 0.5166\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5477 - val_loss: 0.5124\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 78us/step - loss: 0.5472 - val_loss: 0.5209\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5469 - val_loss: 0.5148\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5470 - val_loss: 0.5460\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5465 - val_loss: 0.5119\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5471 - val_loss: 0.5455\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5451 - val_loss: 0.5085\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5475 - val_loss: 0.5202\n",
            "3870/3870 [==============================] - 0s 33us/step\n",
            "[CV]  learning_rate=0.0026242047821147525, n_hidden=0, n_neurons=55, total=  20.9s\n",
            "[CV] learning_rate=0.0026242047821147525, n_hidden=0, n_neurons=55 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 2.8801 - val_loss: 2.5289\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.9010 - val_loss: 0.7396\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.7206 - val_loss: 0.6887\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.6827 - val_loss: 0.7243\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.6640 - val_loss: 0.6647\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.6479 - val_loss: 0.6254\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.6356 - val_loss: 0.6207\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.6233 - val_loss: 0.5980\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.6137 - val_loss: 0.5851\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.6054 - val_loss: 0.5766\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5980 - val_loss: 0.6169\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5895 - val_loss: 0.5607\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5856 - val_loss: 0.6164\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5790 - val_loss: 0.5486\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5755 - val_loss: 0.5452\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5717 - val_loss: 0.5972\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5654 - val_loss: 0.5399\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5675 - val_loss: 0.5349\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5605 - val_loss: 0.5301\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5624 - val_loss: 0.5492\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5571 - val_loss: 0.5500\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5547 - val_loss: 0.5335\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5522 - val_loss: 0.5211\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.5527 - val_loss: 0.5242\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5489 - val_loss: 0.5200\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5517 - val_loss: 0.5672\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5485 - val_loss: 0.5247\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5461 - val_loss: 0.5153\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5471 - val_loss: 0.5134\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5466 - val_loss: 0.5369\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5441 - val_loss: 0.5126\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5454 - val_loss: 0.5666\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5446 - val_loss: 0.5373\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5435 - val_loss: 0.5150\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.5437 - val_loss: 0.5371\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5421 - val_loss: 0.5106\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5424 - val_loss: 0.5090\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5427 - val_loss: 0.5150\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5403 - val_loss: 0.5115\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.5436 - val_loss: 0.5712\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5422 - val_loss: 0.5253\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5405 - val_loss: 0.5082\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5416 - val_loss: 0.5081\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5429 - val_loss: 0.5306\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5407 - val_loss: 0.5178\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5399 - val_loss: 0.5080\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 80us/step - loss: 0.5404 - val_loss: 0.5092\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5414 - val_loss: 0.5403\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5409 - val_loss: 0.5341\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5406 - val_loss: 0.5293\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5388 - val_loss: 0.5098\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.5430 - val_loss: 0.5134\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.5410 - val_loss: 0.5221\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.5390 - val_loss: 0.5082\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.5405 - val_loss: 0.5083\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.5414 - val_loss: 0.5455\n",
            "3870/3870 [==============================] - 0s 42us/step\n",
            "[CV]  learning_rate=0.0026242047821147525, n_hidden=0, n_neurons=55, total=  36.7s\n",
            "[CV] learning_rate=0.0026242047821147525, n_hidden=0, n_neurons=55 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 103us/step - loss: 2.8911 - val_loss: 5.8422\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.8473 - val_loss: 5.3930\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.6562 - val_loss: 5.3891\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.6158 - val_loss: 5.4186\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.5943 - val_loss: 5.4411\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.5770 - val_loss: 5.4813\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 96us/step - loss: 0.5624 - val_loss: 5.5095\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.5537 - val_loss: 5.5216\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5445 - val_loss: 5.4594\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 82us/step - loss: 0.5371 - val_loss: 5.4862\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5307 - val_loss: 5.4698\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 81us/step - loss: 0.5246 - val_loss: 5.4665\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 83us/step - loss: 0.5224 - val_loss: 5.4854\n",
            "3870/3870 [==============================] - 0s 36us/step\n",
            "[CV]  learning_rate=0.0026242047821147525, n_hidden=0, n_neurons=55, total=   9.4s\n",
            "[CV] learning_rate=0.011611005051944592, n_hidden=1, n_neurons=36 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 99us/step - loss: 1.0475 - val_loss: 7.1619\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.6777 - val_loss: 0.5150\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.4821 - val_loss: 0.4296\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4592 - val_loss: 0.4432\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4413 - val_loss: 0.4153\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4310 - val_loss: 0.4155\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4250 - val_loss: 0.3891\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.4198 - val_loss: 0.4069\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4395 - val_loss: 0.3965\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4099 - val_loss: 0.3726\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4068 - val_loss: 0.4360\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4025 - val_loss: 0.3732\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.4048 - val_loss: 0.3920\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.4018 - val_loss: 0.3777\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3946 - val_loss: 0.3603\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3999 - val_loss: 0.3763\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3928 - val_loss: 0.3659\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3914 - val_loss: 0.3723\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3817 - val_loss: 0.3588\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3847 - val_loss: 0.3668\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3804 - val_loss: 0.4552\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3776 - val_loss: 0.3615\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3738 - val_loss: 0.3408\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3754 - val_loss: 0.3493\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 94us/step - loss: 0.3711 - val_loss: 0.4067\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3837 - val_loss: 0.4002\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3697 - val_loss: 0.3405\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3792 - val_loss: 0.3708\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3664 - val_loss: 0.3327\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3678 - val_loss: 0.3349\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3726 - val_loss: 0.3497\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3648 - val_loss: 0.3444\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3607 - val_loss: 0.3354\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3605 - val_loss: 0.3407\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3757 - val_loss: 0.3424\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3647 - val_loss: 0.3278\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3570 - val_loss: 0.3378\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3529 - val_loss: 0.3432\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3618 - val_loss: 0.3349\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3512 - val_loss: 0.3326\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.3527 - val_loss: 0.3949\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3527 - val_loss: 0.3325\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3449 - val_loss: 0.3233\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3483 - val_loss: 0.3252\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3430 - val_loss: 0.4351\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.3587 - val_loss: 0.3267\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3460 - val_loss: 0.4906\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3633 - val_loss: 0.3497\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3463 - val_loss: 0.4381\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3412 - val_loss: 0.3196\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3397 - val_loss: 0.4042\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3515 - val_loss: 0.3386\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3390 - val_loss: 0.4019\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3375 - val_loss: 0.3435\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3389 - val_loss: 0.3315\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3367 - val_loss: 0.4702\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3529 - val_loss: 0.3894\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3468 - val_loss: 0.3330\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3558 - val_loss: 0.3786\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3572 - val_loss: 0.3159\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3341 - val_loss: 0.3740\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3355 - val_loss: 0.3169\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3400 - val_loss: 0.3644\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3343 - val_loss: 0.3186\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3468 - val_loss: 0.3408\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3346 - val_loss: 0.3393\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3320 - val_loss: 0.3413\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3313 - val_loss: 0.3405\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3293 - val_loss: 0.3743\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3308 - val_loss: 0.3266\n",
            "3870/3870 [==============================] - 0s 40us/step\n",
            "[CV]  learning_rate=0.011611005051944592, n_hidden=1, n_neurons=36, total=  49.0s\n",
            "[CV] learning_rate=0.011611005051944592, n_hidden=1, n_neurons=36 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 98us/step - loss: 1.6551 - val_loss: 218.3692\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: nan - val_loss: nan\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: nan - val_loss: nan\n",
            "3870/3870 [==============================] - 0s 37us/step\n",
            "[CV]  learning_rate=0.011611005051944592, n_hidden=1, n_neurons=36, total=   8.1s\n",
            "[CV] learning_rate=0.011611005051944592, n_hidden=1, n_neurons=36 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 95us/step - loss: 0.8977 - val_loss: 0.8894\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.5353 - val_loss: 1.0101\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4762 - val_loss: 0.9905\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.4614 - val_loss: 0.8510\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4352 - val_loss: 0.7451\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 87us/step - loss: 0.4291 - val_loss: 0.6403\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.4109 - val_loss: 0.5638\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.4048 - val_loss: 0.5020\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3998 - val_loss: 0.4417\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.4127 - val_loss: 0.4086\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 86us/step - loss: 0.3901 - val_loss: 0.3989\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3874 - val_loss: 0.4473\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 89us/step - loss: 0.3887 - val_loss: 0.3651\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 85us/step - loss: 0.3865 - val_loss: 0.3801\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 91us/step - loss: 0.3766 - val_loss: 0.3780\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 90us/step - loss: 0.3807 - val_loss: 0.3759\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3754 - val_loss: 0.3829\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3728 - val_loss: 0.3870\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3673 - val_loss: 0.3948\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 88us/step - loss: 0.3671 - val_loss: 0.4157\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 93us/step - loss: 0.3752 - val_loss: 0.4160\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 92us/step - loss: 0.3659 - val_loss: 0.4446\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 84us/step - loss: 0.3643 - val_loss: 0.4297\n",
            "3870/3870 [==============================] - 0s 37us/step\n",
            "[CV]  learning_rate=0.011611005051944592, n_hidden=1, n_neurons=36, total=  16.2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 22.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5ae18773bdf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[1;32m     12\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0;32m--> 736\u001b[0;31m                 **self.best_params_))\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     80\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[1;32m     81\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                (estimator, name))\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <keras.wrappers.scikit_learn.KerasRegressor object at 0x7fb9201b0860>, as the constructor either does not set or modifies parameter learning_rate"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBYj8cu4Vv6Z",
        "colab_type": "text"
      },
      "source": [
        "RandomizedSearchCV uses K-fold cross-validation, so it does not use X_valid and y_valid, which are only used for early stopping.\n",
        "\n",
        "The exploration may last many hours, depending on the hardware, the size of the dataset, the complexity of the model, and the values of n_iter and cv. When it’s over, you can access the best parameters found, the best score, and the trained Keras model like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsV3B4Y3VNns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "259b02c1-26c7-4d06-ad07-ff50b4108bdb"
      },
      "source": [
        "# see best params\n",
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.011366299432595662, 'n_hidden': 2, 'n_neurons': 79}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzoR6lJIV2EX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e143748-243d-46aa-e419-d670a26d7b31"
      },
      "source": [
        "# best scores\n",
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3420590176215776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTu--DAmV5Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "model = rnd_search_cv.best_estimator_.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCS54wr2p3u5",
        "colab_type": "text"
      },
      "source": [
        "There are many techniques to explore a search space much more efficiently than randomly. Their core idea is simple: when a region of the space turns out to be good, it should be explored more. Such techniques take care of the “zooming” process for you and lead to much better solutions in much less time. Here are some Python libraries you can use to optimize hyperparameters:\n",
        "\n",
        "* **[Hyperopt](https://github.com/hyperopt/hyperopt)**\n",
        "* **[Hyperas](https://github.com/maxpumperla/hyperas), [kopt](https://github.com/Avsecz/kopt), [Talos](https://github.com/autonomio/talos)**\n",
        "* **[Keras Tuner](https://www.youtube.com/watch?v=Un0JDL3i5Hg&t=24s)**\n",
        "* **[skopt](https://scikit-optimize.github.io/stable/)**\n",
        "* **[Hyperband](https://github.com/zygmuntz/hyperband)**\n",
        "* **[sklearn-deap](https://github.com/rsteca/sklearn-deap)**\n",
        "* **[Spearmint](https://github.com/JasperSnoek/spearmint)**\n",
        "\n",
        "Hyperparameter tuning is still an active area of research, and evolutionary algorithms are making a comeback. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRfdukzOg7dt",
        "colab_type": "text"
      },
      "source": [
        "### Number of hidden layers\n",
        "For many problems you can start with just one or two hidden layers and the neural network will work just fine. For instance, you can easily reach above 97% accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, and above 98% accuracy using two hidden layers with the same total number of neurons, in roughly the same amount of training time. For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers and they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be a lot faster and require much less data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haDqVoU3qcYT",
        "colab_type": "text"
      },
      "source": [
        "### Number of Neurons per Hidden Layer\n",
        "\n",
        "The number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 × 28 = 784 input neurons and 10 output neurons. As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, and the third with 100. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
        "\n",
        "***In general you will get more bang for your buck by increasing the number of layers instead of the number of neurons per layer.***"
      ]
    }
  ]
}
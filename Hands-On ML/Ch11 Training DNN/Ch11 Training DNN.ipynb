{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch11 Training Deep Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxUbcn-w6zSk",
        "colab_type": "text"
      },
      "source": [
        "# Training Deep Neural Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU4er26vYpym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVLT8Unq603Z",
        "colab_type": "text"
      },
      "source": [
        "## The Vanishing/Exploding Gradients Problems\n",
        "\n",
        "The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
        "\n",
        "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the vanishing gradients problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem, which surfaces in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
        "\n",
        "This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio. The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks).\n",
        "\n",
        "When inputs become large (negative or positive), the logistic activation function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mXaZgphk8c",
        "colab_type": "text"
      },
      "source": [
        "### Glorot and He Initialization\n",
        "\n",
        "Glorot and Bengio, point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs,2 and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction. It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in Equation 11-1, where $fan_{avg} = (fan_{in} + fa_{nout})/2$. This initialization strategy is called Xavier initialization or Glorot initialization, after the paper’s first author.\n",
        "\n",
        "***Glorot initialization (when using the logistic activation function)*** <br>\n",
        "\n",
        "Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
        "\n",
        "Or a uniform distribution between $−r$ and $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$\n",
        "\n",
        "By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\" like this:\n",
        "\n",
        "```\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "```\n",
        "\n",
        "If you want He initialization with a uniform distribution but based on $fan_{avg}$ rather than $fan_{in}$, you can use the VarianceScaling initializer like this:\n",
        "\n",
        "```\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTEUdQB9RX3A",
        "colab_type": "text"
      },
      "source": [
        "### Nonsaturating Activation Functions\n",
        "\n",
        "ReLU activation function is great, but not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative. \n",
        "\n",
        "Another [paper](https://arxiv.org/abs/1505.00853), compares other types of activation functions:\n",
        "* Leaky ReLU\n",
        "* Randomized Leaky ReLU\n",
        "* Parametric Leaky ReLU\n",
        "* Exponential Linear Unit (ELU)\n",
        "* [Scaled ELU](https://arxiv.org/abs/1706.02515)\n",
        "\n",
        "SELU is a scaled variant of the ELU activation function. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones). There are, however, a few conditions for self-normalization to happen (see the paper for the mathematical justification):\n",
        "\n",
        "* The input features must be standardized (mean 0 and standard deviation 1).\n",
        "* Every hidden layer’s weights must be initialized with LeCun normal initialization. In Keras, this means setting **kernel_initializer=\"lecun_normal\"**.\n",
        "* The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
        "* The paper only guarantees self-normalization if all layers are dense, but some\n",
        "researchers have noted that the SELU activation function can improve perfor‐\n",
        "mance in convolutional neural nets as well \n",
        "\n",
        "```\n",
        "NOTE: So, which activation function should you use for the hidden layers of\n",
        "your deep neural networks? Although your mileage will vary, in general SELU > \n",
        "ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network’s \n",
        "architecture prevents it from selfnormalizing, then ELU may perform better than \n",
        "SELU (since SELU is not smooth at z = 0). If you care a lot about runtime \n",
        "latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another \n",
        "hyperparameter, you may use the default α values used by Keras (e.g., 0.3 for \n",
        "leaky ReLU). If you have spare time and computing power, you can use \n",
        "cross-validation to evaluate other activation functions, such as RReLU if your \n",
        "network is overfitting or PReLU if you have a huge training set. That said, \n",
        "because ReLU is the most used activation function (by far), many libraries and \n",
        "hardware accelerators provide ReLU-specific optimizations; therefore, if speed \n",
        "is your priority, ReLU might still be the best choice.\n",
        "```\n",
        "\n",
        "To use the leaky ReLU activation function, create a LeakyReLU layer and add it to your model just after the layer you want to apply it to:\n",
        "```\n",
        "model = keras.models.Sequential([\n",
        "    [...]\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2),\n",
        "    [...]\n",
        "])\n",
        "```\n",
        "For PReLU, replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no official implementation of RReLU in Keras.\n",
        "\n",
        "For SELU activation, set **activation=\"selu\"** and **kernel_initializer=\"lecun_normal\"** when creating a layer:\n",
        "\n",
        "```\n",
        "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1qLrwhaVy9S",
        "colab_type": "text"
      },
      "source": [
        "### Batch Normalization\n",
        "\n",
        "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
        "\n",
        "Batch Normalization consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).\n",
        "\n",
        "So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it’s not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable. One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions. However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer’s input means and standard deviations. This is what Keras does automatically when you use the BatchNormalization layer. To sum up, four parameter vectors are learned in each batch-normalized layer:\n",
        "\n",
        "* γ (the output scale vector)\n",
        "* β (the output offset vector) are learned through regular backpropagation\n",
        "* μ (the final input mean vector) \n",
        "* σ (the final input standard deviation vector) are estimated using an exponential moving average.\n",
        "\n",
        "Note that μ and σ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations)\n",
        "\n",
        "Batch Normalization acts like a regularizer, reducing the need for other regularization techniques.\n",
        "\n",
        "Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it’s often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPPPsUTYVj_",
        "colab_type": "text"
      },
      "source": [
        "#### Implementing Batch Normalization with Keras\n",
        "\n",
        "As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a BatchNormalization layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model. For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFcWMMHxu0Xu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "cfe3420e-3486-4998-e079-5de229a6094d"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAoZEQrwZNKP",
        "colab_type": "text"
      },
      "source": [
        "As you can see, each BN layer adds four parameters per input: γ, β, μ, and σ (for example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two parameters, μ and σ, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable” (if you count the total number of BN parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total number of non-trainable parameters in this model).\n",
        "\n",
        "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWwahsrgY3FZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "0d68c512-f8cd-4b9e-f608-697455fcae26"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization_3/gamma:0', True),\n",
              " ('batch_normalization_3/beta:0', True),\n",
              " ('batch_normalization_3/moving_mean:0', False),\n",
              " ('batch_normalization_3/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hXecX-PZj6H",
        "colab_type": "text"
      },
      "source": [
        "Now when you create a BN layer in Keras, it also creates two operations that will be called by Keras at each iteration during training. These operations will update the moving averages. Since we are using the TensorFlow backend, these operations are TensorFlow operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNqUXretZiVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a7a6c211-d51d-4138-f548-30b2938493a3"
      },
      "source": [
        "model.layers[1].updates"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Operation 'cond/Identity' type=Identity>,\n",
              " <tf.Operation 'cond_1/Identity' type=Identity>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQsL0hlHaFoA",
        "colab_type": "text"
      },
      "source": [
        "The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after (as we just did). There is some debate about this, as which is preferable seems to depend on the task—you can experiment with this too to see which option works best on your dataset. To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass use_bias=False when creating it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg20firnaADq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "1a9ee8f2-b71d-44a4-b343-e552fa0296d8"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "]) \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imekx26UbWdI",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Clipping\n",
        "\n",
        "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called [Gradient Clipping](https://arxiv.org/abs/1211.5063). This technique is most often used in recurrent neural net‐ works, as Batch Normalization is tricky to use in RNNs.\n",
        "\n",
        "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer, like this:\n",
        "```\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "```\n",
        "This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter you can tune. \n",
        "\n",
        "Note that it may change the orientation of the gradient vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. \n",
        "\n",
        "In practice, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than the threshold you picked. \n",
        "\n",
        "For example, if you set clipnorm=1.0, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component. If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEzLKbgdchPt",
        "colab_type": "text"
      },
      "source": [
        "## Reusing Pretrained Layers\n",
        "\n",
        "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then reuse the lower layers of this network. This technique is called transfer learning.\n",
        "\n",
        "It will not only speed up training considerably, but also require significantly less training data. Suppose you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network.\n",
        "\n",
        "The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
        "\n",
        "Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse.\n",
        "\n",
        "Try freezing all the reused layers first (i.e., make their weights non-trainable so that Gradient Descent won’t modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.\n",
        "\n",
        "If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfSz_a4Jd8Sp",
        "colab_type": "text"
      },
      "source": [
        "### Transfer Learning with Keras\n",
        "\n",
        "Suppose the Fashion MNIST dataset only contained eight classes—for example, all the classes except for sandal and shirt. Someone built and trained a Keras model on that set and got reasonably good performance (>90% accuracy). Let’s call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (positive=shirt, negative=sandal). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let’s call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it’s a much easier task (there are just two classes), you were hoping for more. While drinking your morning coffee, you realize that your task is quite similar to task A, so perhaps transfer learning can help? Let’s find out!\n",
        "\n",
        "First, you need to load model A and create a new model based on that model’s layers. Let’s reuse all the layers except for the output layer:\n",
        "```\n",
        "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "```\n",
        "Note that model_A and model_B_on_A now share some layers. When you train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone model_A before you reuse its layers. To do this, you clone model A’s architecture with clone_model(), then copy its weights (since clone_model() does not clone the weights):\n",
        "\n",
        "```\n",
        "model_A_clone = keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())\n",
        "```\n",
        "Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer’s trainable attribute to False and compile the model:\n",
        "\n",
        "```\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "NOTE: You must always compile your model after you freeze or unfreeze layers.\n",
        "\n",
        "Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:\n",
        "\n",
        "```\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
        "validation_data=(X_valid_B, y_valid_B))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbU0sdk8f3Ye",
        "colab_type": "text"
      },
      "source": [
        "### Unsupervised Pretraining\n",
        "\n",
        "Suppose you want to tackle a complex task for which you don’t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don’t lose hope! First, you should try to gather more labeled training data, but if you can’t, you may still be able to perform *unsupervised pretraining*. Indeed, it is often cheap to gather unlabeled training examples, but expensive to label them. If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network. Then you can reuse the lower layers of the autoencoder or the lower layers of the GAN’s discriminator, add the output layer for your task on top, and finetune the final network using supervised learning (i.e., with the labeled training examples).\n",
        "\n",
        "Unsupervised pretraining (today typically using autoencoders or GANs rather than RBMs) is still a good option when you have a complex task to solve, no similar model you can reuse, and little labeled training data but plenty of unlabeled training data.\n",
        "\n",
        "In the early days of Deep Learning it was difficult to train deep models, so people would use a technique called ***greedy layer-wise pretraining***. They would first train an unsupervised model with a single layer, typically an RBM, then they would freeze that layer and add another one on top of it, then train the model again (effectively just training the new layer), then freeze the new layer and add another layer on top of it, train the model again, and so on. Nowadays, things are much simpler: people generally train the full unsupervised model in one shot and use autoencoders or GANs rather than RBMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGj81oO7hjJF",
        "colab_type": "text"
      },
      "source": [
        "### Pretraining on an Auxiliary Task\n",
        "\n",
        "If you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network’s lower layers will learn feature detectors that will likely be reusable by the second neural network.\n",
        "\n",
        "For example, if you want to build a system to recognize faces, you may only have a few pictures of each individual—clearly not enough to train a good classifier. Gathering hundreds of pictures of each person would not be practical. You could, however, gather a lot of pictures of random people on the web and train a first neural network to detect whether or not two different pictures feature the same person. Such a network would learn good feature detectors for faces, so reusing its lower layers would allow you to train a good face classifier that uses little training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O5-oI9wiU3m",
        "colab_type": "text"
      },
      "source": [
        "## Faster Optimizers\n",
        "\n",
        "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): applying a good initialization strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning). Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer. In this section we will present the most popular algorithms: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT4dfPbnip-A",
        "colab_type": "text"
      },
      "source": [
        "### Momentum Optimization\n",
        "\n",
        "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind momentum optimization. In contrast, regular Gradient Descent will simply take small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.\n",
        "\n",
        "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate η), and it updates the weights by adding this momentum vector. In other words, the gradient is used for acceleration, not for speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter β, called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9\n",
        "\n",
        "***Momentum Algorithm*** <br>\n",
        "1. $m \\leftarrow \\beta m - \\eta \\triangledown_{\\theta}J(\\theta) $\n",
        "2. $\\theta \\leftarrow \\theta + m$\n",
        "\n",
        "You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate η multiplied by 1/(1–β) (ignoring the sign). For example, if β = 0.9, then the terminal velocity is equal to 10 times the gradient times the learning rate, so momentum optimization ends up going 10 times faster than Gradient Descent! This allows momentum optimization to escape from plateaus much faster than Gradient Descent. \n",
        "\n",
        "Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons it’s good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence.\n",
        "\n",
        "Implementing momentum in keras is easy:\n",
        "```\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "```\n",
        "\n",
        "The one drawback of momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpyaTiuMNeYv",
        "colab_type": "text"
      },
      "source": [
        "### Nesterov Accelerated Gradient\n",
        "\n",
        "The Nesterov Accelerated Gradient (NAG) method, also known as Nesterov momentum optimization, measures the gradient of the cost function not at the local position θ but slightly ahead in the direction of the momentum, at θ + βm.\n",
        "\n",
        "***Nesterov Accelerated Gradient algorithm*** <br>\n",
        "1. $m \\leftarrow \\beta m - \\eta \\triangledown_{\\theta}J(\\theta + \\beta m) $\n",
        "2. $\\theta \\leftarrow \\theta + m$\n",
        "\n",
        "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position.\n",
        "\n",
        "NAG is generally faster than regular momentum optimization. To use it, simply set nesterov=True when creating the SGD optimizer:\n",
        "```\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JOndzEkOXeo",
        "colab_type": "text"
      },
      "source": [
        "### AdaGrad\n",
        "\n",
        "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The AdaGrad algorithm achieves this correction by scaling down the gradient vector along the steepest dimensions. \n",
        "\n",
        "This algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum.\n",
        "\n",
        "AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an Adagrad optimizer, **you should not use it to train deep neural networks** (it may be efficient for simpler tasks such as Linear Regression, though). Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmHze4hFR3ye",
        "colab_type": "text"
      },
      "source": [
        "### RMSProp\n",
        "\n",
        "As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. The RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training).\n",
        "\n",
        "As you might expect, Keras has an RMSprop optimizer:\n",
        "```\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
        "```\n",
        "Note that the rho argument corresponds to β in Equation 11-7. Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HwJ1ikISgpk",
        "colab_type": "text"
      },
      "source": [
        "### Adam and Nadam Optimization\n",
        "\n",
        "Adam, which stands for adaptive moment estimation, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients.\n",
        "\n",
        "Here is how to create an Adam optimizer using Keras:\n",
        "```\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "```\n",
        "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter η. You can often use the default value η = 0.001, making Adam even easier to use than Gradient Descent.\n",
        "\n",
        "***Nadam*** optimization is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. In his report introducing this technique,19 the researcher Timothy Dozat compares many different optimizers on various tasks and finds that Nadam generally outperforms Adam but is sometimes outperformed by RMSProp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6-BVbpjXtRC",
        "colab_type": "text"
      },
      "source": [
        "## Avoiding Overfitting Through Regularization\n",
        "\n",
        "Deep neural networks typically have tens of thousands of parameters, sometimes even millions. This gives them an incredible amount of freedom and means they can fit a huge variety of complex datasets. But this great flexibility also makes the network prone to overfitting the training set. We need regularization.\n",
        "\n",
        "We already implemented one of the best regularization techniques, early stopping. Moreover, even though Batch Normalization was designed to solve the unstable gradients problems, it also acts like a pretty good regularizer. In this section we will examine other popular regularization techniques for neural networks: ℓ1 and ℓ2 regularization, dropout, and max-norm regularization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ppEpKUoYO0w",
        "colab_type": "text"
      },
      "source": [
        "### ℓ1 and ℓ2 Regularization\n",
        "\n",
        "You can use ℓ2 regularization to constrain a neural network’s connection weights, and/or ℓ1 regularization if you want a sparse model (with many weights equal to 0). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights, using a regularization factor of 0.01:\n",
        "```\n",
        "layer = keras.layers.Dense(100, activation=\"elu\", \n",
        "                           kernel_initializer=\"he_normal\", \n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "```\n",
        "\n",
        "The l2() function returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss. As you might expect, you can just use keras.regularizers.l1() if you want ℓ1 regularization; if you want both ℓ1 and ℓ2 regularization, use keras.regularizers.l1_l2() (specifying both regularization factors).\n",
        "\n",
        "Since you will typically want to apply the same regularizer to all layers in your network, as well as using the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments. This makes the code ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is to use Python’s functools.partial() function, which lets you create a thin wrapper for any callable, with some default argument values:\n",
        "```\n",
        "from functools import partial\n",
        "\n",
        "RegularizedDense = partial(keras.layers.Dense, \n",
        "                           activation=\"elu\", \n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\",\n",
        "                     kernel_initializer=\"glorot_uniform\")\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOYhMpSEZ7z3",
        "colab_type": "text"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Dropout is one of the most popular regularization techniques for deep neural networks. It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter p is called the dropout rate, and it is typically set between 10% and 50%: closer to 20– 30% in recurrent neural nets, and closer to 40–50% in convolutional neural networks. After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily). \n",
        "\n",
        "It’s surprising at first that this destructive technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows; perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them. The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better. \n",
        "\n",
        "There is one small but important technical detail. Suppose p = 50%, in which case during testing a neuron would be connected to twice as many input neurons as it would be (on average) during training. To compensate for this fact, we need to multiply each neuron’s input connection weights by 0.5 after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on and will be unlikely to perform well. More generally, we need to multiply each input connection weight by the keep probability (1 – p) after training. Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well).\n",
        "\n",
        "To implement dropout using Keras, you can use the keras.layers.Dropout layer. During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability. After training, it does nothing at all; it just passes the inputs to the next layer. The following code applies dropout regularization before every Dense layer, using a dropout rate of 0.2:\n",
        "\n",
        "```\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "```\n",
        "\n",
        "***WARNING*** - Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training).\n",
        "\n",
        "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BQDhVzCeno6",
        "colab_type": "text"
      },
      "source": [
        "### Max-Norm Regularization\n",
        "\n",
        "Another regularization technique that is popular for neural networks is called maxnorm regularization: for each neuron, it constrains the weights w of the incoming connections such that $∥ w ∥_2 ≤ r$, where r is the max-norm hyperparameter and $∥ · ∥_2$ is the ℓ2 norm. Max-norm regularization does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing ∥w∥2 after each training step and rescaling w if needed $(w ← w r/∥ w ∥_2)$.\n",
        "\n",
        "To implement max-norm regularization in Keras, set the kernel_constraint argument of each hidden layer to a max_norm() constraint with the appropriate max value, like this:\n",
        "```\n",
        "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", \n",
        "                   kernel_constraint=keras.constraints.max_norm(1.))\n",
        "```\n",
        "After each training iteration, the model’s fit() method will call the object returned by max_norm(), passing it the layer’s weights and getting rescaled weights in return, which then replace the layer’s weights. You can define your own custom constraint function if necessary and use it as the kernel_con straint. You can also constrain the bias terms by setting the bias_constraint argument.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7c5GI7hgDSC",
        "colab_type": "text"
      },
      "source": [
        "# Practical Guidelines (refer book)"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch4 Training Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUNF5VVOPFv-",
        "colab_type": "text"
      },
      "source": [
        "# Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5APGvZp4PIUk",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression\n",
        "Linear Regression Equation - $ \\hat{y} = \\theta_0 + \\theta_1 x_1 + ...+\\theta_n x_n$\n",
        "\n",
        "In this equation:\n",
        "* ŷ is the predicted value.\n",
        "* n is the number of features.\n",
        "* $x_i$ is the ith feature value.\n",
        "* $θ_j$ is the jth model parameter (including the bias term θ0 and the feature weights $θ_1, θ_2, ..., θ_n)$.\n",
        "\n",
        "Vectorized Linear Regression Eqn - $\\hat{y} = h_\\theta(x) = \\theta.X$\n",
        "\n",
        "In this equation:\n",
        "* θ is the model’s parameter vector, containing the bias term $θ_0$ and the feature weights $θ_1$ to $θ_n$.\n",
        "* x is the instance’s feature vector, containing $x_0$ to $x_n$, with $x_0$ always equal to 1.\n",
        "* $θ · X$ is the dot product of the vectors θ and x, which is of course equal to $θ_0 x_0 + θ_1 x_1 + ... + θ_n x_n$.\n",
        "* $h_θ$ is the hypothesis function, using the model parameters θ.\n",
        "\n",
        "MSE cost function for Linear Regression model <br>\n",
        "$ MSE(X, h_\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(\\theta^T x^{(i)} - y^{(i)})^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRrJ-MQRT62N",
        "colab_type": "text"
      },
      "source": [
        "### The Normal Equation\n",
        "To find the value of θ that minimizes the cost function, there is a closed-form solution —in other words, a mathematical equation that gives the result directly. This is called the Normal Equation <br>\n",
        "$\\hat{\\theta} = (X^T X)^{-1} X^T y$\n",
        "\n",
        "In this equation:\n",
        "* $\\hat{\\theta}$ is the value of θ that minimizes the cost function.\n",
        "* y is the vector of target values containing $y^{(1)}$ to $y^{(m)}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4eAiNwrAUuT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "bd8cb2ba-f45f-4ee2-e90a-8621a17776ec"
      },
      "source": [
        "# Let’s generate some linear-looking data to test this equation\n",
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X, y)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxcCYPlyVLJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4d7c83ec-1fcf-487b-9308-2f1db2d3d693"
      },
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
        "# inv() to calculate inverse and .dot() for matrix multiplication\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "print(theta_best)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.98110009]\n",
            " [2.97465505]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3rTxPb0Vci0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dfba7753-8de4-48f1-e991-eaad201471eb"
      },
      "source": [
        "# let's make some predictions\n",
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
        "y_predict = X_new_b.dot(theta_best)\n",
        "y_predict"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.98110009],\n",
              "       [9.93041018]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr80rwj-WcCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "4fd0d1e0-8497-4f29-e209-9a8f9de61ffb"
      },
      "source": [
        "# plotting the regression line\n",
        "plt.plot(X_new, y_predict, \"r-\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ivoi8_PWm49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c29c8c5d-b349-4316-d7b0-8e4fd8badea4"
      },
      "source": [
        "# LINEAR REGRESSION with SCIKIT LEARN\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "print(f\"lin_reg.intercept_, lin_reg.coef_ = {lin_reg.intercept_, lin_reg.coef_}\")\n",
        "print(f\"lin_reg.predict(X_new) - {lin_reg.predict(X_new)}\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lin_reg.intercept_, lin_reg.coef_ = (array([3.98110009]), array([[2.97465505]]))\n",
            "lin_reg.predict(X_new) - [[3.98110009]\n",
            " [9.93041018]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhbHNQJYXlTn",
        "colab_type": "text"
      },
      "source": [
        "The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vlmcgurXPlt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e886d694-cb32-4fd4-a2d9-35755682a180"
      },
      "source": [
        "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
        "theta_best_svd"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.98110009],\n",
              "       [2.97465505]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdGlYbMGYoyD",
        "colab_type": "text"
      },
      "source": [
        "This function computes θ = $X^+$y, where $X^+$ is the pseudoinverse of X (specifically, the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_RxbFFyXqEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f682a43b-e0e3-4d75-f1ae-00fdd6664155"
      },
      "source": [
        "np.linalg.pinv(X_b).dot(y)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.98110009],\n",
              "       [2.97465505]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WuHjMH_ZJ6r",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFb7txP9ZeZ2",
        "colab_type": "text"
      },
      "source": [
        "### Batch Gradient Descent\n",
        "Partial Derivative of cost function<br>\n",
        "$\\frac{\\partial }{\\partial \\theta_j} MSE(\\theta = \\frac{2}{m} \\sum_{i=1}^{m}(\\theta^T x^{(i)} - y^{(i)})x_j^{(i)})$\n",
        "\n",
        "Gradient Descent Step<br>\n",
        "$\\theta^{(next step)} = \\theta - \\eta \\nabla_\\theta MSE(\\theta)$\n",
        "\n",
        "It involves calculations over the full training set X, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step (actually, Full Gradient Descent would probably be a better name). As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition.\n",
        "\n",
        "Let’s look at a quick implementation of this algorithm:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdEVmkrqZAld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8d1b57eb-efa9-44ed-9f69-ed922ae659c9"
      },
      "source": [
        "eta = 0.1 # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "theta = np.random.randn(2,1) # random initialization\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "print(theta)    "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.98110009]\n",
            " [2.97465505]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0raDw0g0mYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "89bf131e-111f-4174-b5ad-d5bdc6250ab1"
      },
      "source": [
        "# let's try with different leraning rates\n",
        "\n",
        "etas = [0.002, # too small, takes too many iterations to converge\n",
        "        0.1,  # just right\n",
        "        0.5   # too large, will jump around\n",
        "        ]\n",
        "\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "theta = np.random.randn(2,1) # random initialization\n",
        "for eta in etas:\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "        theta = theta - eta * gradients\n",
        "    print(f\"eta: {eta} \\ntheta = {theta}\\n\\n\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eta: 0.002 \n",
            "theta = [[3.25313941]\n",
            " [3.63230152]]\n",
            "\n",
            "\n",
            "eta: 0.1 \n",
            "theta = [[3.98110009]\n",
            " [2.97465505]]\n",
            "\n",
            "\n",
            "eta: 0.5 \n",
            "theta = [[3.98110009]\n",
            " [2.97465505]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhPEr4V41-ae",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic Gradient descent\n",
        "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously, working on a single instance at a time makes the algorithm much faster because it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.\n",
        "\n",
        "Stochastic Gradient Descent using a simple learning schedule:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB8r22P41Txs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "34ca91ca-932c-4312-844f-cb9d3e9e4e29"
      },
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
        "\n",
        "def learning_schedule(t):\n",
        "    \"\"\" Used to decrease lr\"\"\"\n",
        "    return t0 / (t + t1)\n",
        "    \n",
        "theta = np.random.randn(2,1) # random initialization\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "\n",
        "print(theta)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.0302585 ]\n",
            " [2.95585978]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShtRHkXo3NLQ",
        "colab_type": "text"
      },
      "source": [
        "To perform Linear Regression using Stochastic GD with Scikit-Learn, you can use the SGDRegressor class, which defaults to optimizing the squared error cost function. The following code runs for maximum 1,000 epochs or until the loss drops by less than 0.001 during one epoch (max_iter=1000, tol=1e-3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsjtTe5N2_Qb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d9e57ed-40aa-4d27-9b8a-e22e3351e6eb"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "print(sgd_reg.intercept_, sgd_reg.coef_)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.96133949] [2.97373929]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBEpVPEJ3XUh",
        "colab_type": "text"
      },
      "source": [
        "### Mini Batch Gradient Descent\n",
        "At each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vYBMhXv5IOo",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial Regression \n",
        "What if your data is more complex than a straight line? Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.\n",
        "\n",
        "Let's look at an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2cckhiO3VFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "b4bbc0d7-876c-4963-e338-4d54d148e4c2"
      },
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn3HRNBP6Rmd",
        "colab_type": "text"
      },
      "source": [
        "Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s Poly nomialFeatures class to transform our training data, adding the square (second degree polynomial) of each feature in the training set as a new feature (in this case there is just one feature):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4qM3rLZ5nD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "943b5ccd-75cc-48e0-cde8-5c26d24c9f93"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "print(X[0])\n",
        "print(X_poly[0])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.8747498]\n",
            "[-0.8747498  0.7651872]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOD2QDUu6mlT",
        "colab_type": "text"
      },
      "source": [
        "X_poly now contains the original feature of X plus the square of this feature. Now you can fit a LinearRegression model to this extended training data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzAemBYE6T2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5b67197-5f26-4c78-c2c7-97a2e1c22f45"
      },
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "print(lin_reg.intercept_, lin_reg.coef_)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.24333671] [[0.92305153 0.47212725]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vsTa5lq7HQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "4b5280fb-2976-46af-87cd-7c607cc97175"
      },
      "source": [
        "# prediction on first 5 train samples\n",
        "lin_reg.predict(np.squeeze([np.c_[a, a**2] for a in X[:5]]))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.7971633 ],\n",
              "       [3.65865155],\n",
              "       [1.94704741],\n",
              "       [2.39415553],\n",
              "       [1.81685899]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVIJifPG_6so",
        "colab_type": "text"
      },
      "source": [
        "## Regularized Linear Models\n",
        "A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. A simple way to regularize a polynomial model is to reduce\n",
        "the number of polynomial degrees.\n",
        "\n",
        "For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg-ys0ATAeT0",
        "colab_type": "text"
      },
      "source": [
        "### Ridge Regression\n",
        "Ridge Regression is a regularized version of Linear Regression: a regularization term equal to $\\alpha \\sum_{i=1}^{n} \\theta_i^2$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized performance measure to evaluate the model’s performance.\n",
        "\n",
        "Cost Function: <br>\n",
        "$J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\sum_{i=1}^{n} \\theta_i^2$\n",
        "\n",
        "Bias term $\\theta_0$ is not regularized.\n",
        "\n",
        "Ridge Regression Closed Form: <br>\n",
        "$\\hat{\\theta} = (X^T X + \\alpha A)^{-1} X^T y$\n",
        "\n",
        "Lets look at an example, using normal form:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMdEXIqe7-H7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b7f1d96-3171-4775-b067-34591ef46d5b"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.25288096]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvmx2-uUH5iq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8632ad2f-efb5-4622-f78e-ec3b15d88c6a"
      },
      "source": [
        "# Example using SGD\n",
        "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.24486247])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gqg1473INT2",
        "colab_type": "text"
      },
      "source": [
        "The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function equal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYcO0P0uIOR2",
        "colab_type": "text"
      },
      "source": [
        "### Lasso Regression\n",
        "Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm of the weight vector instead of half the square of the ℓ2 norm.\n",
        "\n",
        "Cost Function:<br>\n",
        "$J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\sum_{i=1}^{n} |\\theta_i|$\n",
        "\n",
        "An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero). In other words, **Lasso Regression automatically performs feature selection and outputs a sparse model** (i.e., with few nonzero feature weights). \n",
        "\n",
        "Let's have a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WFt9RsQIHY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fd8ae95-0a34-4880-d830-9d84224c78f8"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "lasso_reg.predict([[1.5]])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.21448093])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9pOLPorMU-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "515dcae6-2e1d-4489-fc10-0c0b57b9d73b"
      },
      "source": [
        "# Example using SGD\n",
        "sgd_reg = SGDRegressor(penalty=\"l1\")\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.2412248])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7xq4MLyPVe4",
        "colab_type": "text"
      },
      "source": [
        "### Elastic Net\n",
        "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression.\n",
        "\n",
        "Cost Function: <br>\n",
        "$ J(\\theta) = MSE(\\theta) \n",
        "+ r \\alpha \\sum_{i=1}^n |\\theta_i|\n",
        "+ \\frac{1-r}{2}\\alpha \\sum_{i=1}^{n} |\\theta_i|$\n",
        "\n",
        "Here is a short example that uses Scikit-Learn’s ElasticNet (l1_ratio corresponds to the mix ratio r):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHOgVfJvObxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "233525b2-4223-4e91-ed8c-4ac23048fed9"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "elastic_net.fit(X, y)\n",
        "elastic_net.predict([[1.5]])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.21300664])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CoeSYlhWZt0",
        "colab_type": "text"
      },
      "source": [
        "### Early Stopping\n",
        "A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Here is an example:\n",
        "```Python\n",
        "from sklearn.base import clone\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# prepare the data\n",
        "poly_scaler = Pipeline(\n",
        "    [\n",
        "     (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "     (\"std_scaler\", StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, \n",
        "                       tol=-np.infty, \n",
        "                       warm_start=True, \n",
        "                       penalty=None, \n",
        "                       learning_rate=\"constant\", \n",
        "                       eta0=0.0005)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = clone(sgd_reg)\n",
        "```\n",
        "\n",
        "Note that with warm_start=True, when the fit() method is called it continues train‐ ing where it left off, instead of restarting from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FjAgPuYYlFY",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression\n",
        "Logistic Regression (also called Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to the negative class, labeled “0”). This makes it a binary classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TtyHFWlaL-l",
        "colab_type": "text"
      },
      "source": [
        "### Estimating Probabilities\n",
        "Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the logistic of this result. \n",
        "\n",
        "Logistic Regression model estimated probability:<br>\n",
        "$\\hat{p} = h_\\theta (x) = \\sigma(x^T \\theta)$\n",
        "\n",
        "$\\sigma(.)$ is sigmod function that outputs a number between 0 and 1.\n",
        "\n",
        "Logistic Function:<br>\n",
        "$\\sigma(t) =  \\frac{1}{1 + \\exp^{(-t)}}$\n",
        "\n",
        "Once the Logistic Regression model has estimated the probability $\\hat{p}= h_θ(x)$ that an instance x belongs to the positive class, it can make its prediction ŷ easily.\n",
        "\n",
        "Logistic Regression model prediction:<br>\n",
        "$\\hat{y} = \\left\\{ \\begin{array}{rl}\n",
        " 0 &\\mbox{ if $\\hat{p}$ < 0.5 } \\\\\n",
        " 1 &\\mbox{ if $\\hat{p} \\geq $  0.5 }\n",
        "\\end{array} \\right. $\n",
        "\n",
        "σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\n",
        "model predicts 1 if x⊺ θ is positive and 0 if it is negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYKq4TmjnMvK",
        "colab_type": "text"
      },
      "source": [
        "### Training and Cost Function\n",
        "The objective of Logistic Regression training is to set the parameter vector θ so that the model estimates high probabilities for positive instances (y = 1) and low probabilities for negative instances (y = 0). This idea is captured by the cost function for a single training instance x.\n",
        "\n",
        "Cost Function of single training instance:<br>\n",
        "$\n",
        "c(\\theta) = \\left\\{ \n",
        "    \\begin{array}{rl}\n",
        "        -log(\\hat{p}) &\\mbox{ if y = 1 } \\\\\n",
        "        -log(1-\\hat{p}) &\\mbox{ if y = 0 }\n",
        "    \\end{array} \\right. \n",
        "$\n",
        "\n",
        "This cost function makes sense because –log(t) grows very large when t approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, and it will also be very large if the model estimates a probability close to 1 for a negative instance. On the other hand, –log(t) is close to 0 when t is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.\n",
        "\n",
        "The cost function over the whole training set is the average cost over all training instances. It can be written in a single expression called the log loss.\n",
        "\n",
        "Log Loss: <br>\n",
        "$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [ y^{(i)} log(\\hat{p}^{(i)}) + (1-y^{(i)}) log(1-\\hat{p}^{(i)})]$\n",
        "\n",
        "The bad news is that there is no known closed-form equation to compute the value of θ that minimizes this cost function (there is no equivalent of the Normal Equation). The good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough).\n",
        "\n",
        "Logistic cost function partial derivatives: <br>\n",
        "$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\sigma(\\theta^T x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
        "\n",
        "For each instance it computes the prediction error and multiplies it by the $j^{th}$ feature value, and then it computes the average over all training instances. Once you have the gradient vector containing all the partial derivatives, you can use it in the Batch Gradient Descent algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1EzlWZ7XK69",
        "colab_type": "text"
      },
      "source": [
        "### Decision Boundaries\n",
        "Let’s use the iris dataset to illustrate Logistic Regression and try to build a classifier to detect the Iris virginica type based only on the petal width feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thV01zFqXJH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f028a278-764e-489c-e300-15cd3001745e"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(iris.keys())\n",
        "print(iris['DESCR'])\n",
        "X = iris[\"data\"][:, 3:] # petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int) # 1 if Iris virginica, else 0"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            "        - sepal length in cm\n",
            "        - sepal width in cm\n",
            "        - petal length in cm\n",
            "        - petal width in cm\n",
            "        - class:\n",
            "                - Iris-Setosa\n",
            "                - Iris-Versicolour\n",
            "                - Iris-Virginica\n",
            "                \n",
            "    :Summary Statistics:\n",
            "\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "                    Min  Max   Mean    SD   Class Correlation\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: 33.3% for each of 3 classes.\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "     Structure and Classification Rule for Recognition in Partially Exposed\n",
            "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "     on Information Theory, May 1972, 431-433.\n",
            "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "     conceptual clustering system finds 3 classes in the data.\n",
            "   - Many, many more ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNNZRgwsx7Gw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "280d19d9-3b32-42d9-e0ff-210aac2826dc"
      },
      "source": [
        "# logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X, y)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmYoPcK945ZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7b70f9df-dec6-4d42-b199-a0c346115d0a"
      },
      "source": [
        "# model’s estimated probabilities for flowers with petal widths varying from 0 cm to 3 cm\n",
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris virginica\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8LosGbL5xhj",
        "colab_type": "text"
      },
      "source": [
        "### Softmax Regression\n",
        "The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.\n",
        "\n",
        "When given an instance x, the Softmax Regression model first computes a score $s_k(x)$ for each class k, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores. The equation to compute $s_k(x)$ should look familiar, as it is just like the equation for Linear Regression prediction \n",
        "\n",
        "Softmax score for class k:<br>\n",
        "$s_k(x) = x^T \\theta^{(k)}$\n",
        "\n",
        "Each class has its own dedicated parameter vector $θ^{(k)}$. All these vectors are typically stored as rows in a parameter matrix Θ.\n",
        "\n",
        "Once you have computed the score of every class for the instance x, you can estimate the probability pk that the instance belongs to class k by running the scores through the softmax function. The function computes the exponential of every score, then normalizes them (dividing by the sum of all the exponentials). The scores are generally called logits or log-odds (although they are actually unnormalized log-odds).\n",
        "\n",
        "Softmax Function:<br>\n",
        "$\\hat{p} = \\sigma(s(x))_k = \\frac{\\exp{(s_k(x))}}{\\sum_{j=1}^K \\exp{(s_k(x))}}$\n",
        "\n",
        "In this Equation:\n",
        "* K is the number of classes.\n",
        "* s(x) is a vector containing the scores of each class for the instance x.\n",
        "* $\\sigma(s(x))_k$ is the estimated probability that the instance x belongs to class k, given the scores of each class for that instance.\n",
        "\n",
        "Just like the Logistic Regression classifier, the Softmax Regression classifier predicts the class with the highest estimated probability.\n",
        "\n",
        "Softmax Regression classifier prediction:<br>\n",
        "$\\hat{y} = \\underset{k}{argmax} \\sigma(s(x))_k = \\underset{k}{argmax} s(x)_k = \\underset{k}{argmax} ((\\theta^{(k)})^Tx)$\n",
        "\n",
        "The argmax operator returns the value of a variable that maximizes a function. In this equation, it returns the value of k that maximizes the estimated probability $\\sigma(s(x))_k$.\n",
        "\n",
        "The objective of Softmax Regression is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). Minimizing the cost function, called the cross entropy, should lead to this objective because it penalizes the model when it estimates a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities matches the target classes.\n",
        "\n",
        "Cross entropy Cost function:<br>\n",
        "$J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^{K} y_k^{(i)} log(\\hat{p}_k^{(i)})$\n",
        "\n",
        "Here, $y_k^{(i)}$ is the target probability that the ith instance belongs to class k. In general, it is either equal to 1 or 0, depending on whether the instance belongs to the class or not. When K=2, this cost function is equivalent to the Logistic Regression’s cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsHf9e1dAe4a",
        "colab_type": "text"
      },
      "source": [
        "Cross entropy gradient vector for class k:<br>\n",
        "$ \\nabla_{\\theta^{(k)}} J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^m (\\hat{p}_k^{(i)} - y_k^{(i)})x^{(i)}$\n",
        "\n",
        "Now you can compute the gradient vector for every class, then use Gradient Descent (or any other optimization algorithm) to find the parameter matrix Θ that minimizes the cost function.\n",
        "\n",
        "Let’s use Softmax Regression to classify the iris flowers into all three classes. ScikitLearn’s LogisticRegression uses one-versus-the-rest by default when you train it on more than two classes, but you can set the multi_class hyperparameter to \"multino mial\" to switch it to Softmax Regression. You must also specify a solver that supports Softmax Regression, such as the \"lbfgs\" solver (see Scikit-Learn’s documentation for more details). It also applies ℓ2 regularization by default, which you can control using the hyperparameter C:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fy-LLCx48vQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "7d859005-6249-4ae4-8070-3f407acccebc"
      },
      "source": [
        "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
        "softmax_reg.fit(X, y)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEHKOHn3B7bp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2df78e22-f64c-465c-8f2b-01186c4cd7de"
      },
      "source": [
        "# Predict petal 5cm long 2cm wide\n",
        "print(f\"class - {softmax_reg.predict([[5, 2]])}\")\n",
        "print(f\"Probabs - {softmax_reg.predict_proba([[5, 2]])}\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class - [2]\n",
            "Probabs - [[6.38014896e-07 5.74929995e-02 9.42506362e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
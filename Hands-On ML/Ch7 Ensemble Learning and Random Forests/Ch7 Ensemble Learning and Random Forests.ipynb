{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch7 Ensemble Learning and Random Forests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YedIx23W2PO7",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble Learning and Random Forests\n",
        "In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.\n",
        "\n",
        "As an example of an Ensemble method, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you obtain the predictions of all the individual trees, then predict the class that gets the most votes. Such an ensemble of Decision Trees is called a Random Forest, and despite its simplicity, this is one of the most powerful Machine Learning algorithms available today.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRT9hkMrQR-N",
        "colab_type": "text"
      },
      "source": [
        "## Voting Classifiers\n",
        "Suppose you have trained a few classifiers, each one achieving about 80% accuracy. \n",
        "\n",
        "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a ***hard voting*** classifier. \n",
        "\n",
        "This voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a ***weak learner*** (meaning it does only slightly better than random guessing), the ensemble can still be a ***strong learner*** (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.\n",
        "\n",
        "How is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased coin that has a 51% chance of coming up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get more or less 510 heads and 490 tails, and hence a majority of heads. If you do the math, you will find that the probability of obtaining a majority of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This is due to the ***law of large numbers***: as you keep tossing the coin, the ratio of heads gets closer and closer to the probability of heads (51%).\n",
        "\n",
        "The following code creates and trains a voting classifier in Scikit-Learn, composed of three diverse classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5UQiAhL2Kvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGU8QfdGWI2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make data\n",
        "X_train, y_train = make_moons(n_samples=10000, noise=0.25)\n",
        "X_test, y_test = make_moons(n_samples=1000, noise=0.25)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ejrQNmWF__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "75049ee4-ca6e-4abd-f224-ab56d86c7359"
      },
      "source": [
        "# make classifiers\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr',\n",
              "                              LogisticRegression(C=1.0, class_weight=None,\n",
              "                                                 dual=False, fit_intercept=True,\n",
              "                                                 intercept_scaling=1,\n",
              "                                                 l1_ratio=None, max_iter=100,\n",
              "                                                 multi_class='auto',\n",
              "                                                 n_jobs=None, penalty='l2',\n",
              "                                                 random_state=None,\n",
              "                                                 solver='lbfgs', tol=0.0001,\n",
              "                                                 verbose=0, warm_start=False)),\n",
              "                             ('rf',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     cr...\n",
              "                                                     oob_score=False,\n",
              "                                                     random_state=None,\n",
              "                                                     verbose=0,\n",
              "                                                     warm_start=False)),\n",
              "                             ('svc',\n",
              "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                  class_weight=None, coef0=0.0,\n",
              "                                  decision_function_shape='ovr', degree=3,\n",
              "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                  probability=False, random_state=None,\n",
              "                                  shrinking=True, tol=0.001, verbose=False))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzBVN8b3UGYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c405c4ab-823a-4e1b-b35a-74eca16b6389"
      },
      "source": [
        "# checking each classifier score\n",
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.853\n",
            "RandomForestClassifier 0.935\n",
            "SVC 0.935\n",
            "VotingClassifier 0.936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIbL9vw4V9WI",
        "colab_type": "text"
      },
      "source": [
        "If all classifiers are able to estimate class probabilities (i.e., they all have a pre dict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace voting=\"hard\" with voting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is not the case for the SVC class by default, so you need to set its probability hyperparameter to True (this will make the SVC class use cross-validation to estimate class probabilities, slowing down training, and it will add a predict_proba() method). It may increase the accuracy. Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA_IDAXiUvLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "de7cd1f3-47c0-418c-8a40-8bb7e70b7145"
      },
      "source": [
        "svm_clf = SVC(probability=True)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# checking each classifier score\n",
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.853\n",
            "RandomForestClassifier 0.934\n",
            "SVC 0.935\n",
            "VotingClassifier 0.936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpInZX-7edFf",
        "colab_type": "text"
      },
      "source": [
        "## Bagging and Pasting\n",
        "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.\n",
        "\n",
        "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrZHdzixq-n_",
        "colab_type": "text"
      },
      "source": [
        "### Bagging and Pasting in Scikit-Learn\n",
        "Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas sifier class (or BaggingRegressor for regression). The following code trains an ensemble of 500 Decision Tree classifiers:5 each is trained on 100 training instances randomly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions (–1 tells Scikit-Learn to use all available cores):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v193J7KDXMQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "eae6c06f-b0c5-4255-9ac4-03acca98577f"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),\n",
        "    n_estimators=500,\n",
        "    max_samples=100, \n",
        "    bootstrap=True, \n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bag_clf.fit(X_train, y_train)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                        class_weight=None,\n",
              "                                                        criterion='gini',\n",
              "                                                        max_depth=None,\n",
              "                                                        max_features=None,\n",
              "                                                        max_leaf_nodes=None,\n",
              "                                                        min_impurity_decrease=0.0,\n",
              "                                                        min_impurity_split=None,\n",
              "                                                        min_samples_leaf=1,\n",
              "                                                        min_samples_split=2,\n",
              "                                                        min_weight_fraction_leaf=0.0,\n",
              "                                                        presort='deprecated',\n",
              "                                                        random_state=None,\n",
              "                                                        splitter='best'),\n",
              "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
              "                  max_samples=100, n_estimators=500, n_jobs=-1, oob_score=False,\n",
              "                  random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeN5rscTs2W2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c07eb2e-b2c4-4718-af08-bb9386ea70db"
      },
      "source": [
        "y_pred = bag_clf.predict(X_test)\n",
        "print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BaggingClassifier 0.932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7Gx7ixtYmH",
        "colab_type": "text"
      },
      "source": [
        "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBbrDcodtjN3",
        "colab_type": "text"
      },
      "source": [
        "### Out of Bag Evaluation\n",
        "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the training set. Those instances which are not sampled are called ***out-of-bag (oob) instances***, and may be different for exery predictor.\n",
        "\n",
        "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation score is available through the oob_score_ variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usURsbLcs21H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90b21167-4984-4752-bf2e-8b4990f138b0"
      },
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), \n",
        "    n_estimators=500,\n",
        "    bootstrap=True, \n",
        "    n_jobs=-1, \n",
        "    oob_score=True\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9396"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU3zl55Pu5qF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "417648f5-5894-46c2-eae3-01e782c08f26"
      },
      "source": [
        "# this accuracy score will be around same that of previous cell\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print(f\"Accuracy Score - {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score - 0.935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw8iC59Fvb19",
        "colab_type": "text"
      },
      "source": [
        "The oob decision function for each training instance is also available through the ***oob_decision_function_*** variable. In this case (since the base estimator has a ***predict_proba()*** method), the decision function returns the class probabilities for each training instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPRWFXp5vGvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "5618bde7-10b2-4f98-a231-90399c1ca89d"
      },
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.88709677, 0.11290323],\n",
              "       [1.        , 0.        ],\n",
              "       [0.81325301, 0.18674699],\n",
              "       ...,\n",
              "       [0.13756614, 0.86243386],\n",
              "       [0.4591195 , 0.5408805 ],\n",
              "       [1.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5JPn2vQwVJi",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest\n",
        "Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees. The following code uses all available CPU cores to train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQZjOFhnvkZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "bca99dde-eebf-46bf-a8e7-6825d81cfa0c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=16, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
              "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9woQ6aZR29Gr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98a7cef6-6f9d-4f43-da1f-fc3281b2d51e"
      },
      "source": [
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "print(f\"Accuracy Score - {accuracy_score(y_test, y_pred_rf)}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score - 0.941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL98jCXv3fwy",
        "colab_type": "text"
      },
      "source": [
        "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:\n",
        "```Python\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
        "    n_estimators=500, \n",
        "    max_samples=1.0, \n",
        "    bootstrap=True, \n",
        "    n_jobs=-1\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0otRdwX83vIe",
        "colab_type": "text"
      },
      "source": [
        "### Extra-Trees\n",
        "When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).\n",
        "\n",
        "A forest of such extremely random trees is called an Extremely Randomized Trees ensemble (or Extra-Trees for short). Once again, this technique trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.\n",
        "\n",
        "You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API as the RandomForestRegressor class.\n",
        "\n",
        "NOTE: It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters using grid search)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_shIPrx6QwC",
        "colab_type": "text"
      },
      "source": [
        "### Feature Importance\n",
        "Another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it.\n",
        "\n",
        "Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can access the result using the feature_importances_ variable. For example, the following code trains a RandomForestClassifier on the iris dataset and outputs each feature’s importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK7zLxzO3fGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a91b7551-76ad-4c40-bef1-d693cc18eb55"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "    print(name, score)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sepal length (cm) 0.10461223801878682\n",
            "sepal width (cm) 0.025190125233975603\n",
            "petal length (cm) 0.4351317341136877\n",
            "petal width (cm) 0.43506590263355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpAFtTEM-UTH",
        "colab_type": "text"
      },
      "source": [
        "## Boosting\n",
        "Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are AdaBoost (short for Adaptive Boosting) and Gradient Boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hTlGrsBAy7Q",
        "colab_type": "text"
      },
      "source": [
        "### Ada Boost\n",
        "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predic‐ tors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
        "\n",
        "For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such as a Decision Tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on.\n",
        "\n",
        "Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
        "\n",
        "Let’s take a closer look at the AdaBoost algorithm. Each instance weight $w^{(i)}$ is initially set to $1/m$. A first predictor is trained, and its weighted error rate $r_1$ is computed on the training set.\n",
        "\n",
        "***Weighted error rate of the $j^{th}$ predictor*** <br>\n",
        "$\n",
        "r_j = \\frac{\\underset{\\hat{y}_j^{(i)} \\neq y_j^{(i)}}{\\sum_{i=1}^m w^{(i)}} }{\\sum_{i=1}^m w^{(i)}}\n",
        "$\n",
        "\n",
        "where $\\hat{y}_j^{(i)}$ is the jth predictor’s prediction for the ith instance.\n",
        "\n",
        "The predictor’s weight αj is then computed, where η is the learning rate hyperparameter (defaults to 1). The more accurate the predictor is, the higher its weight will be. If it is just guessing randomly, then its weight will be close to zero. However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.\n",
        "\n",
        "***Predictor weight*** <br>\n",
        "$\n",
        "\\alpha_j = \\eta log\\frac{1 - r_j}{r_j}\n",
        "$\n",
        "\n",
        "Next, the AdaBoost algorithm updates the instance weights, which boosts the weights of the misclassified instances.\n",
        "\n",
        "***Weight update rule*** <br>\n",
        "for i = 1, 2, ..., m <br>\n",
        "$\n",
        "w^{(i)} = \\left\\{ \\begin{array}{rl}\n",
        "w^{(i)} &\\mbox{ if $\\hat{y}_j^{(i)} = y_j^{(i)}$} \\\\\n",
        "w^{(i)}\\exp{(\\alpha_j)} &\\mbox{ if $\\hat{y}_j^{(i)} \\neq y_j^{(i)}$}\n",
        "       \\end{array} \\right.\n",
        "$\n",
        "\n",
        "Then all the instance weights are normalized.\n",
        "\n",
        "Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
        "\n",
        "To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights $\\alpha_j$. The predicted class is the one that receives the majority of weighted votes.\n",
        "\n",
        "Scikit-Learn uses a multiclass version of AdaBoost called ***SAMME (which stands for Stagewise Additive Modeling using a Multiclass Exponential loss function)***. When there are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate class probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use a variant of **SAMME** called **SAMME.R** (the R stands for “Real”), which relies on class probabilities rather than predictions and generally performs better.\n",
        "\n",
        "The following code trains an AdaBoost classifier based on 200 Decision Stumps using Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an Ada BoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1 — in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by2w1vyM3BPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "d09df7b1-dde8-451c-8ca4-f3a28062e8d9"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=200,\n",
        "    algorithm='SAMME.R',\n",
        "    learning_rate=0.5,\n",
        ")\n",
        "\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                         class_weight=None,\n",
              "                                                         criterion='gini',\n",
              "                                                         max_depth=1,\n",
              "                                                         max_features=None,\n",
              "                                                         max_leaf_nodes=None,\n",
              "                                                         min_impurity_decrease=0.0,\n",
              "                                                         min_impurity_split=None,\n",
              "                                                         min_samples_leaf=1,\n",
              "                                                         min_samples_split=2,\n",
              "                                                         min_weight_fraction_leaf=0.0,\n",
              "                                                         presort='deprecated',\n",
              "                                                         random_state=None,\n",
              "                                                         splitter='best'),\n",
              "                   learning_rate=0.5, n_estimators=200, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BX34lnbLS41",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boost\n",
        "Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
        "\n",
        "Let’s go through a simple regression example, using Decision Trees as the base predictors (of course, Gradient Boosting also works great with regression tasks). This is called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic training set):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR_dOsMjOKgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data prep\n",
        "import numpy as np\n",
        "\n",
        "X = np.linspace(-50, 50, 500)\n",
        "noise = np.random.normal(0, 100, size=X.shape)\n",
        "y = X**2 + noise"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdEmm9gbS-Sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "220f4933-0be6-4b56-8638-88659235e61a"
      },
      "source": [
        "# print data samples\n",
        "print(X[:5])\n",
        "print(noise[:5])\n",
        "print(y[:5])"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-50.         -49.7995992  -49.5991984  -49.3987976  -49.19839679]\n",
            "[  9.53421027 142.43804235  67.54777529   8.82506354  79.99082845]\n",
            "[2509.53421027 2622.43812267 2527.6282569  2449.06626739 2500.47307551]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNky9ipDQIei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f9d67364-7da9-4994-83e8-cae35723deef"
      },
      "source": [
        "# plot data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X, y)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f9c573a1a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXjOuhJ9T4rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, X_new, y, y_new = train_test_split(X, y, test_size=0.33)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldxfBsiLUipD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "4b9be13c-f8e5-422c-c00f-58e4ca3b2c40"
      },
      "source": [
        "# reshape data, according to input requirement\n",
        "X = X.reshape((-1, 1))\n",
        "X_new = X_new.reshape((-1, 1))\n",
        "\n",
        "# Sanity check\n",
        "print(f\"X.shape - {X.shape}\")\n",
        "print(f\"y.shape - {y.shape}\")\n",
        "print(f\"X_new.shape - {X_new.shape}\")\n",
        "print(f\"y_new.shape - {y_new.shape}\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape - (335, 1)\n",
            "y.shape - (335,)\n",
            "X_new.shape - (165, 1)\n",
            "y_new.shape - (165,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpNeTpI4K-T5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "fc2265b5-cff2-4755-ba86-76e00846c85c"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X, y)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmZZNOrvL1qU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "95a68d4a-7c7b-4684-ba39-406765ebbf75"
      },
      "source": [
        "# train a second DecisionTreeRegressor on the residual errors made by the first predictor\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X, y2)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmLstRIDMUcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "7119619f-fd56-4270-c75a-26ce1cac57ef"
      },
      "source": [
        "# train a third DecisionTreeRegressor on the residual errors made by the first predictor\n",
        "y3 = y2 - tree_reg1.predict(X)\n",
        "\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X, y3)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAYNsQonMg8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "abc31aca-93b7-4004-af4a-f9c622aec989"
      },
      "source": [
        "# Now we have an ensemble containing three trees. It can make predictions on a new\n",
        "# instance simply by adding up the predictions of all the trees:\n",
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
        "print(f\"MSE - {mean_squared_error(y_new, y_pred)}\")\n",
        "print(f\"MAE - {mean_absolute_error(y_new, y_pred)}\")"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE - 1018833.5413112247\n",
            "MAE - 797.3878906435757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC6zm1xBYFfV",
        "colab_type": "text"
      },
      "source": [
        "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe gressor class. Much like the RandomForestRegressor class, it has hyperparameters to control the growth of Decision Trees (e.g., max_depth, min_samples_leaf), as well as hyperparameters to control the ensemble training, such as the number of trees (n_estimators). The following code creates the same ensemble as the previous one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b23Pmv7_M2SY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "ada5da5e-8f8a-4555-81c9-d32406cc81e3"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt.fit(X, y)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
              "                          init=None, learning_rate=1.0, loss='ls', max_depth=2,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, n_estimators=3,\n",
              "                          n_iter_no_change=None, presort='deprecated',\n",
              "                          random_state=None, subsample=1.0, tol=0.0001,\n",
              "                          validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJDElumfYRX_",
        "colab_type": "text"
      },
      "source": [
        "The **learning_rate** hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐ ing set, but the predictions will usually generalize better. This is a regularization tech‐ nique called ***shrinkage***.\n",
        "\n",
        "GBRT may underfit if it does not have sufficient number of trees and overfit if it has too many trees.In order to find the optimal number of trees, you can use early stopping . A simple way to implement this is to use the staged_predict() method: it returns an iterator over the predictions made by the ensemble at each stage of training (with one tree, two trees, etc.). The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHLvNN83YP40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1fc68555-6a61-4070-87fb-2f3bb8a93511"
      },
      "source": [
        "# n_estimators: the number of boosting stages to perform\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X, y)\n",
        "\n",
        "# get errors, and find minimum error among all these stages\n",
        "errors = [mean_squared_error(y_new, y_pred) for y_pred in gbrt.staged_predict(X_new)]\n",
        "print(f\"Errors:\")\n",
        "for ind, error in enumerate(errors):\n",
        "    print(f\"Index - {ind}\\MSE - {error}\")\n",
        "bst_n_estimators = np.argmin(errors) + 1\n",
        "print(f\"bst_n_estimators - {bst_n_estimators}\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Errors:\n",
            "Index - 0\\MSE - 448867.4334867611\n",
            "Index - 1\\MSE - 384619.7518546116\n",
            "Index - 2\\MSE - 334145.7594560882\n",
            "Index - 3\\MSE - 286958.82265370135\n",
            "Index - 4\\MSE - 250235.35830303511\n",
            "Index - 5\\MSE - 220221.20697357357\n",
            "Index - 6\\MSE - 193365.78861617885\n",
            "Index - 7\\MSE - 168590.6675658507\n",
            "Index - 8\\MSE - 150879.2263217608\n",
            "Index - 9\\MSE - 135264.56440844337\n",
            "Index - 10\\MSE - 119175.68064056354\n",
            "Index - 11\\MSE - 106234.11718813228\n",
            "Index - 12\\MSE - 95622.310251313\n",
            "Index - 13\\MSE - 87729.39712116508\n",
            "Index - 14\\MSE - 78095.67462133266\n",
            "Index - 15\\MSE - 70786.01236492129\n",
            "Index - 16\\MSE - 63510.88010654962\n",
            "Index - 17\\MSE - 57134.84631575513\n",
            "Index - 18\\MSE - 52737.766809555666\n",
            "Index - 19\\MSE - 47441.29514894984\n",
            "Index - 20\\MSE - 44273.80570921537\n",
            "Index - 21\\MSE - 40409.53729169354\n",
            "Index - 22\\MSE - 36624.24087362093\n",
            "Index - 23\\MSE - 34363.47596787056\n",
            "Index - 24\\MSE - 31485.526783547888\n",
            "Index - 25\\MSE - 29993.9510101917\n",
            "Index - 26\\MSE - 28580.666566756143\n",
            "Index - 27\\MSE - 26192.830042261947\n",
            "Index - 28\\MSE - 24923.694101029636\n",
            "Index - 29\\MSE - 23783.48985817747\n",
            "Index - 30\\MSE - 22233.85383849539\n",
            "Index - 31\\MSE - 20995.381286196138\n",
            "Index - 32\\MSE - 20341.547108775743\n",
            "Index - 33\\MSE - 19812.041205471476\n",
            "Index - 34\\MSE - 18685.6501370444\n",
            "Index - 35\\MSE - 17670.635370559547\n",
            "Index - 36\\MSE - 17225.987999235735\n",
            "Index - 37\\MSE - 16689.89292238705\n",
            "Index - 38\\MSE - 16348.599022721657\n",
            "Index - 39\\MSE - 15773.480490369524\n",
            "Index - 40\\MSE - 15585.449273746517\n",
            "Index - 41\\MSE - 15050.764563153043\n",
            "Index - 42\\MSE - 14846.258149614296\n",
            "Index - 43\\MSE - 14586.957602100108\n",
            "Index - 44\\MSE - 14244.614916598899\n",
            "Index - 45\\MSE - 13967.171005629189\n",
            "Index - 46\\MSE - 13887.708977310489\n",
            "Index - 47\\MSE - 13807.630121838101\n",
            "Index - 48\\MSE - 13534.43933049263\n",
            "Index - 49\\MSE - 13293.009564947633\n",
            "Index - 50\\MSE - 13256.988354899791\n",
            "Index - 51\\MSE - 13224.871149527276\n",
            "Index - 52\\MSE - 13120.225284527902\n",
            "Index - 53\\MSE - 13065.692599431934\n",
            "Index - 54\\MSE - 13055.334103302033\n",
            "Index - 55\\MSE - 12960.762338923343\n",
            "Index - 56\\MSE - 12924.60908522926\n",
            "Index - 57\\MSE - 12908.480835770348\n",
            "Index - 58\\MSE - 12857.614407793264\n",
            "Index - 59\\MSE - 12784.934613033489\n",
            "Index - 60\\MSE - 12774.131055914684\n",
            "Index - 61\\MSE - 12768.335451746538\n",
            "Index - 62\\MSE - 12770.553142127203\n",
            "Index - 63\\MSE - 12729.096888834758\n",
            "Index - 64\\MSE - 12698.771327786704\n",
            "Index - 65\\MSE - 12718.545688542015\n",
            "Index - 66\\MSE - 12698.670103278286\n",
            "Index - 67\\MSE - 12692.666655648432\n",
            "Index - 68\\MSE - 12701.555818409033\n",
            "Index - 69\\MSE - 12711.939404798943\n",
            "Index - 70\\MSE - 12714.435410939008\n",
            "Index - 71\\MSE - 12707.541516355353\n",
            "Index - 72\\MSE - 12747.132854058565\n",
            "Index - 73\\MSE - 12743.972813185477\n",
            "Index - 74\\MSE - 12742.568265733013\n",
            "Index - 75\\MSE - 12735.100721195577\n",
            "Index - 76\\MSE - 12739.809602313748\n",
            "Index - 77\\MSE - 12729.836460295244\n",
            "Index - 78\\MSE - 12730.598121133271\n",
            "Index - 79\\MSE - 12709.790633764436\n",
            "Index - 80\\MSE - 12709.988935238256\n",
            "Index - 81\\MSE - 12665.578747537944\n",
            "Index - 82\\MSE - 12669.764850459354\n",
            "Index - 83\\MSE - 12654.930158070723\n",
            "Index - 84\\MSE - 12663.901703211941\n",
            "Index - 85\\MSE - 12661.773656443873\n",
            "Index - 86\\MSE - 12655.934762725792\n",
            "Index - 87\\MSE - 12656.65391510455\n",
            "Index - 88\\MSE - 12652.857014823701\n",
            "Index - 89\\MSE - 12652.511693098495\n",
            "Index - 90\\MSE - 12667.98626133396\n",
            "Index - 91\\MSE - 12657.7019403268\n",
            "Index - 92\\MSE - 12661.68802129631\n",
            "Index - 93\\MSE - 12691.48039602772\n",
            "Index - 94\\MSE - 12698.36504622452\n",
            "Index - 95\\MSE - 12703.622711927579\n",
            "Index - 96\\MSE - 12693.386938774458\n",
            "Index - 97\\MSE - 12692.713968315069\n",
            "Index - 98\\MSE - 12700.326909759942\n",
            "Index - 99\\MSE - 12711.170131143375\n",
            "Index - 100\\MSE - 12705.224288070089\n",
            "Index - 101\\MSE - 12701.438782820134\n",
            "Index - 102\\MSE - 12703.607241144551\n",
            "Index - 103\\MSE - 12666.549498858449\n",
            "Index - 104\\MSE - 12662.355782219465\n",
            "Index - 105\\MSE - 12677.750558091122\n",
            "Index - 106\\MSE - 12693.65977877613\n",
            "Index - 107\\MSE - 12670.032586031653\n",
            "Index - 108\\MSE - 12685.338633256122\n",
            "Index - 109\\MSE - 12681.783906317303\n",
            "Index - 110\\MSE - 12676.908648789724\n",
            "Index - 111\\MSE - 12696.326304611985\n",
            "Index - 112\\MSE - 12702.06358337446\n",
            "Index - 113\\MSE - 12718.74745020438\n",
            "Index - 114\\MSE - 12718.93546909453\n",
            "Index - 115\\MSE - 12720.908509719482\n",
            "Index - 116\\MSE - 12756.03364159825\n",
            "Index - 117\\MSE - 12783.264504758115\n",
            "Index - 118\\MSE - 12801.022620233829\n",
            "Index - 119\\MSE - 12803.05171363728\n",
            "bst_n_estimators - 90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbKRYPOQaflm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "d7982a10-0b69-411a-e099-f997d2c4a300"
      },
      "source": [
        "# retrain model with n_estimators=bst_n_estimators\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X, y)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
              "                          init=None, learning_rate=0.1, loss='ls', max_depth=2,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, n_estimators=90,\n",
              "                          n_iter_no_change=None, presort='deprecated',\n",
              "                          random_state=None, subsample=1.0, tol=0.0001,\n",
              "                          validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBJ7uiF9bvAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3b6f48bf-922f-4aa1-c4fa-fecc944950ba"
      },
      "source": [
        "# test on Val data\n",
        "preds = gbrt_best.predict(X_new)\n",
        "print(f\"MSE - {mean_squared_error(y_new, preds)}\")\n",
        "print(f\"MAE - {mean_absolute_error(y_new, preds)}\")"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE - 12652.511693098495\n",
            "MAE - 86.61742379276654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hQFrQSGcBUt",
        "colab_type": "text"
      },
      "source": [
        "It is also possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). You can do so by setting ***warm_start=True***, which makes ScikitLearn keep existing trees when the fit() method is called, allowing incremental training. The following code stops training when the validation error does not improve for five iterations in a row:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a497bYjbc_EV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93f2eab2-79e5-49e5-8cdf-f7ef94c9edbf"
      },
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    gbrt.fit(X, y)\n",
        "    y_pred = gbrt.predict(X_new)\n",
        "    val_error = mean_squared_error(y_new, y_pred)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break # early stopping\n",
        "\n",
        "print(n_estimators)            "
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOKBn0SetSl",
        "colab_type": "text"
      },
      "source": [
        "The GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies the fraction of training instances to be used for training each tree. For example, if ***subsample=0.25***, then each tree is trained on 25% of the training instances, selected randomly. As you can probably guess by now, this technique trades a higher bias for a lower variance. It also speeds up training considerably. This is called ***Stochastic Gradient Boosting***.\n",
        "\n",
        "It is worth noting that an optimized implementation of Gradient Boosting is available in the popular Python library XGBoost, which stands for Extreme Gradient Boosting. In fact, XGBoost is often an important component of the winning entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW000kfReAQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "77170b8e-a100-44eb-da7f-aeabbed8d211"
      },
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X, y)\n",
        "y_pred = xgb_reg.predict(X_new)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16:23:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N61oy7eRfJdL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07acde81-b36d-43ce-c783-bb0b3195594f"
      },
      "source": [
        "# xgb with early stopping\n",
        "xgb_reg.fit(X, y,\n",
        "            eval_set=[(X_new, y_new)], \n",
        "            early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_new)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16:24:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:987.399\n",
            "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
            "[1]\tvalidation_0-rmse:900.499\n",
            "[2]\tvalidation_0-rmse:819.934\n",
            "[3]\tvalidation_0-rmse:747.232\n",
            "[4]\tvalidation_0-rmse:682.676\n",
            "[5]\tvalidation_0-rmse:626.7\n",
            "[6]\tvalidation_0-rmse:572.599\n",
            "[7]\tvalidation_0-rmse:524.309\n",
            "[8]\tvalidation_0-rmse:480.362\n",
            "[9]\tvalidation_0-rmse:441.104\n",
            "[10]\tvalidation_0-rmse:406.228\n",
            "[11]\tvalidation_0-rmse:374.701\n",
            "[12]\tvalidation_0-rmse:344.708\n",
            "[13]\tvalidation_0-rmse:318.556\n",
            "[14]\tvalidation_0-rmse:294.42\n",
            "[15]\tvalidation_0-rmse:272.614\n",
            "[16]\tvalidation_0-rmse:254.466\n",
            "[17]\tvalidation_0-rmse:238.011\n",
            "[18]\tvalidation_0-rmse:222.802\n",
            "[19]\tvalidation_0-rmse:209.171\n",
            "[20]\tvalidation_0-rmse:197.565\n",
            "[21]\tvalidation_0-rmse:186.608\n",
            "[22]\tvalidation_0-rmse:177.867\n",
            "[23]\tvalidation_0-rmse:169.665\n",
            "[24]\tvalidation_0-rmse:162.785\n",
            "[25]\tvalidation_0-rmse:156.302\n",
            "[26]\tvalidation_0-rmse:151.433\n",
            "[27]\tvalidation_0-rmse:146.755\n",
            "[28]\tvalidation_0-rmse:142.401\n",
            "[29]\tvalidation_0-rmse:138.646\n",
            "[30]\tvalidation_0-rmse:135.357\n",
            "[31]\tvalidation_0-rmse:132.637\n",
            "[32]\tvalidation_0-rmse:130.18\n",
            "[33]\tvalidation_0-rmse:127.843\n",
            "[34]\tvalidation_0-rmse:125.747\n",
            "[35]\tvalidation_0-rmse:124.472\n",
            "[36]\tvalidation_0-rmse:122.923\n",
            "[37]\tvalidation_0-rmse:121.892\n",
            "[38]\tvalidation_0-rmse:121.02\n",
            "[39]\tvalidation_0-rmse:120.339\n",
            "[40]\tvalidation_0-rmse:119.732\n",
            "[41]\tvalidation_0-rmse:119.039\n",
            "[42]\tvalidation_0-rmse:118.52\n",
            "[43]\tvalidation_0-rmse:118.071\n",
            "[44]\tvalidation_0-rmse:117.814\n",
            "[45]\tvalidation_0-rmse:117.454\n",
            "[46]\tvalidation_0-rmse:117.334\n",
            "[47]\tvalidation_0-rmse:117.114\n",
            "[48]\tvalidation_0-rmse:116.917\n",
            "[49]\tvalidation_0-rmse:116.744\n",
            "[50]\tvalidation_0-rmse:116.615\n",
            "[51]\tvalidation_0-rmse:116.554\n",
            "[52]\tvalidation_0-rmse:116.653\n",
            "[53]\tvalidation_0-rmse:116.608\n",
            "Stopping. Best iteration:\n",
            "[51]\tvalidation_0-rmse:116.554\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp0EHcEmfXHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "703fcd15-f5da-4bb7-a0ee-d536891ae145"
      },
      "source": [
        "print(f\"MSE - {mean_squared_error(y_new, y_pred)}\")\n",
        "print(f\"MAE - {mean_absolute_error(y_new, y_pred)}\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE - 13584.731835216935\n",
            "MAE - 88.91328956321627\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch8 Dimensionality Reduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsK1793-Cz2G",
        "colab_type": "text"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lErdVerG1RH",
        "colab_type": "text"
      },
      "source": [
        "## The Curse of Dimensionality\n",
        "Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only do all these features make training extremely slow, but they can also make it much harder to find a good solution, as we will see. This problem is often referred to as the curse of dimensionality.\n",
        "\n",
        "In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features (significantly fewer than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpdSoNaFLQ2a",
        "colab_type": "text"
      },
      "source": [
        "## Main Approaches for Dimensionality Reduction\n",
        "Before we dive into specific dimensionality reduction algorithms, let’s take a look at the two main approaches to reducing dimensionality: projection and Manifold Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41kWjjWWLof9",
        "colab_type": "text"
      },
      "source": [
        "### Projection\n",
        "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. \n",
        "\n",
        "If that's the case, we can simply project the datapoints from this dimension to lower ones. However, projection is not always the best approach to dimensionality reduction. In many cases the subspace may twist and turn, such as in the famous Swiss roll toy dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVkELmqOMnN6",
        "colab_type": "text"
      },
      "source": [
        "### Manifold Learning\n",
        "The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.\n",
        "\n",
        "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "021VBOd_Ov-O",
        "colab_type": "text"
      },
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "Principal Component Analysis (PCA) is by far the most popular dimensionality reduc‐ tion algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTklzTLePAcN",
        "colab_type": "text"
      },
      "source": [
        "### Preserving the Variance\n",
        "Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. It is reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIqtBqocQXEj",
        "colab_type": "text"
      },
      "source": [
        "### Principal Components\n",
        "PCA identifies the axis that accounts for the largest amount of variance in the training set. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
        "\n",
        "NOTE: For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the opposite direction as the original vectors. However, they will generally still lie on the same axes. In some cases, a pair of unit vectors may even rotate or swap (if the variances along these two axes are close), but the plane they define will generally remain the same.\n",
        "\n",
        "So how can you find the principal components of a training set? Luckily, there is a standard matrix factorization technique called ***Singular Value Decomposition (SVD)*** that can decompose the training set matrix $X$ into the matrix multiplication of three matrices $U, Σ, V^T$, where $V$ contains the unit vectors that define all the principal components that we are looking for.\n",
        "\n",
        "The following Python code uses NumPy’s svd() function to obtain all the principal components of the training set, then extracts the two unit vectors that define the first two PCs:\n",
        "\n",
        "```Python\n",
        "import numpy as np\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "c1 = Vt.T[:, 0]\n",
        "c2 = Vt.T[:, 1]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egYptuTaWqW6",
        "colab_type": "text"
      },
      "source": [
        "NOTE: PCA assumes that the dataset is centered around the origin. Scikit-Learn’s PCA classes take care of centering the data. If you implement PCA yourself (as in the preceding example), or if you use other libraries, don’t forget to center the data first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQl7ThoISqHe",
        "colab_type": "text"
      },
      "source": [
        "### Projecting Down to d Dimensions\n",
        "\n",
        "Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\n",
        "\n",
        "To project the training set onto the hyperplane and obtain a reduced dataset $X_{d-proj}$ of dimensionality d, compute the matrix multiplication of the training set matrix $X$ by the matrix $W_d$, defined as the matrix containing the first d columns of $V$.\n",
        "\n",
        "***Projecting the training set down to d dimensions*** <br>\n",
        "$X_{d-proj} = X W_d$\n",
        "\n",
        "The following Python code projects the training set onto the plane defined by the first two principal components:\n",
        "```Python\n",
        "W2 = Vt.T[:, :2]\n",
        "X2D = X_centered.dot(W2)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5FyPnSBUXpf",
        "colab_type": "text"
      },
      "source": [
        "### Using Scikit-Learn\n",
        "\n",
        "Scikit-Learn’s PCA class uses SVD decomposition to implement PCA, just like we did earlier in this chapter. The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data):\n",
        "\n",
        "```Python\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 2)\n",
        "X2D = pca.fit_transform(X)\n",
        "```\n",
        "\n",
        "After fitting the PCA transformer to the dataset, its components_ attribute holds the transpose of Wd (e.g., the unit vector that defines the first principal component is equal to pca.components_.T[:, 0])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIGk02iHX7Y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "73d223c5-1fd5-4015-d8d4-d38f8cf9a218"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "print(f\"X.shape - {X.shape}\")\n",
        "print(f\"y.shape - {y.shape}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape - (70000, 784)\n",
            "y.shape - (70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keq91zloXy2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c6de7cc-a601-4c18-f643-71079e9bbece"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 2)\n",
        "X2D = pca.fit_transform(X)\n",
        "print(f\"X2D.shape - {X2D.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X2D.shape - (70000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvs9K_LZUeZz",
        "colab_type": "text"
      },
      "source": [
        "### Explained Variance Ratio\n",
        "Another useful piece of information is the explained variance ratio of each principal component, available via the explained_variance_ratio_ variable. The ratio indicates the proportion of the dataset’s variance that lies along each principal component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT67oGpgYZRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09a80f43-87a2-47e2-bc12-d8169ff00a16"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09746116, 0.07155445])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaqjsZtZVOJF",
        "colab_type": "text"
      },
      "source": [
        "### Choosing the Right Number of Dimensions\n",
        "\n",
        "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for data visualization—in that case you will want to reduce the dimensionality down to 2 or 3.\n",
        "\n",
        "The following code performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set’s variance:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOATB44_CqEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d2a35d59-3760-4ae8-af30-eb87b8b10539"
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "print(f\"d - {d}\")\n",
        "print(f\"pca.explained_variance_ratio_ SUM - {np.sum(pca.explained_variance_ratio_)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d - 154\n",
            "pca.explained_variance_ratio_ SUM - 1.0000000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSCKQg_EY9KW",
        "colab_type": "text"
      },
      "source": [
        "You could then set **n_components=d** and run PCA again. But there is a much better option: instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hUc51t9YteR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5e9271f-4d0b-41c8-c29a-c2ecee6696dd"
      },
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "print(f\"pca.explained_variance_ratio_ SUM - {np.sum(pca.explained_variance_ratio_)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pca.explained_variance_ratio_ SUM - 0.9503499702078614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O6mAx3IZGUl",
        "colab_type": "text"
      },
      "source": [
        "Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot cumsum). There will usually be an elbow in the curve, where the explained variance stops growing fast. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FesKARNQZ29q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "21cdd369-9da8-4734-cab4-1e9c95f30861"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(cumsum)\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiBLMI1maG6y",
        "colab_type": "text"
      },
      "source": [
        "### PCA for Compression\n",
        "\n",
        "After dimensionality reduction, the training set takes up much less space. As an example, try applying PCA to the MNIST dataset while preserving 95% of its variance. You should find that each instance will have just over 150 features, instead of the original 784 features. So, while most of the variance is preserved, the dataset is now less than 20% of its original size! This is a reasonable compression ratio, and you can see how this size reduction can speed up a classification algorithm (such as an SVM classifier) tremendously.\n",
        "\n",
        "It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. This won’t give you back the original data, since the projection lost a bit of information (within the 5% var‐ iance that was dropped), but it will likely be close to the original data. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error.\n",
        "\n",
        "Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEYl370IZ7Fa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0843ebd4-f4bd-45b2-aea3-0abda87323d2"
      },
      "source": [
        "X_recovered = pca.inverse_transform(X_reduced)\n",
        "print(f\"X_recovered.shape - {X_recovered.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_recovered.shape - (70000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WNjxzfScuto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "3ad39fcd-75d3-4e3b-c5ac-cfe268a41254"
      },
      "source": [
        "# plot original data\n",
        "plt.imshow(X[0].reshape((28, -1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f47e6ae9a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2_rFqPycymz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a35b32cc-bcfb-47bb-c426-5ce3e6367c56"
      },
      "source": [
        "# plot recovered data\n",
        "plt.imshow(X_recovered[0].reshape((28, -1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f47e6a53fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI4rO0XDdv08",
        "colab_type": "text"
      },
      "source": [
        "### Randomized PCA\n",
        "If you set the **svd_solver** hyperparameter to \"randomized\", Scikit-Learn uses a stochastic algorithm called Randomized PCA that quickly finds an approximation of the first d principal components. Its computational complexity is $O(m × d^2) + O(d^3)$, instead of $O(m × n^2) + O(n^3)$ for the full SVD approach, so it is dramatically faster than full SVD when d is much smaller than n:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mY5l9-1dfFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c439dbb7-87cc-49fc-b2e8-3bebbfdcf301"
      },
      "source": [
        "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
        "X_reduced = rnd_pca.fit_transform(X)\n",
        "print(f\"X_reduced.shape - {X_reduced.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_reduced.shape - (70000, 154)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH2YgJeGePdX",
        "colab_type": "text"
      },
      "source": [
        "By default, svd_solver is actually set to \"auto\": Scikit-Learn automatically uses the randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m or n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full SVD, you can set the svd_solver hyperparameter to \"full\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI82wtdVfPYF",
        "colab_type": "text"
      },
      "source": [
        "### Incremental PCA\n",
        "One problem with the preceding implementations of PCA is that they require the whole training set to fit in memory in order for the algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have been developed. They allow you to split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is useful for large training sets and for applying PCA online (i.e., on the fly, as new instances arrive). \n",
        "\n",
        "The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5 to reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like before). Note that you must call the partial_fit() method with each mini-batch, rather than the fit() method with the whole training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBe9w0cteIhp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7564d8b6-6be9-41e8-cddc-f2632815c057"
      },
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(X, n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "    X_reduced = inc_pca.transform(X)\n",
        "print(f\"X_reduced.shape - {X_reduced.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_reduced.shape - (70000, 154)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ew2S5Djg2bo",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, you can use NumPy’s *memmap* class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it. Since the IncrementalPCA class uses only a small part of the array at any given time, the memory usage remains under control. This makes it possible to call the usual fit() method, as you can see in the following code:\n",
        "```Python\n",
        "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
        "\n",
        "batch_size = m // n_batches\n",
        "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
        "inc_pca.fit(X_mm)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyO9tiPghHEo",
        "colab_type": "text"
      },
      "source": [
        "## Kernel PCA\n",
        "\n",
        "A mathematical technique that implicitly maps instances into a very high-dimensional space (called the feature space), enabling nonlinear classification and regression with Support Vector Machines. A linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space.\n",
        "\n",
        "It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called Kernel PCA (kPCA). It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n",
        "\n",
        "The following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an RBF kernel.\n",
        "```Python\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "print(f\"X_reduced.shape - {X_reduced.shape}\")  \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEVuenjAjIXg",
        "colab_type": "text"
      },
      "source": [
        "### Selecting a Kernel and Tuning Hyperparameters\n",
        "As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel and hyperparameter values. That said, dimensionality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can use grid search to select the kernel and hyperparameters that lead to the best performance on that task. The following code creates a twostep pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification. Then it uses GridSearchCV to find the best kernel and gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline:\n",
        "\n",
        "```Python \n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"kpca\", KernelPCA(n_components=2)),\n",
        "    (\"log_reg\", LogisticRegression())\n",
        "])\n",
        "param_grid = [{\n",
        "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
        "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
        "}]\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqrl4gE_pnJv",
        "colab_type": "text"
      },
      "source": [
        "## t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
        "\n",
        "It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. See [docs](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
        "\n",
        "```Python\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "print(f\"X_tsne.shape - {X_tsne.shape}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emNfKmOsz4CU",
        "colab_type": "text"
      },
      "source": [
        "## Linear Discriminant Analysis (LDA)\n",
        "\n",
        "A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. See [docs](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRO_7tTkz3it",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc5e1973-8e29-445a-b805-8e134157accb"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "print(f\"X_lda.shape - {X_lda.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_lda.shape - (70000, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}